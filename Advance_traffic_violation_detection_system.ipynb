{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfsoEdzW154IaqL8H5DB3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavankalyan369/Advance-Traffic-Violation-Detection-System/blob/main/Advance_traffic_violation_detection_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helmet Detection**"
      ],
      "metadata": {
        "id": "S2O3bqo4-6ZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZV7PmQcY-NpO",
        "outputId": "d3c4c953-4a81-4d84-efa4-0b24dcafd78e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics -q  # Install ultralytics in this cell\n",
        "!pip install ultralytics -q\n",
        "from ultralytics import YOLO\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "'/content/drive/MyDrive/Tv/helmetdet'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NoYqwNU4_GYE",
        "outputId": "5ee2f039-d71e-4944-f3f2-eda9e588a57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Tv/helmetdet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the YAML content in a file in Colab\n",
        "yaml_content = \"\"\"\n",
        "path: '/content/drive/MyDrive/Tv/helmetdet'\n",
        "train: train/images\n",
        "val: valid/images\n",
        "\n",
        "names:\n",
        "  0: helmet\n",
        "  1: no-helmet\n",
        "\"\"\"\n",
        "\n",
        "with open('data.yaml', 'w') as f:\n",
        "    f.write(yaml_content)\n"
      ],
      "metadata": {
        "id": "WVrpZ-hw_No2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8n.pt')  # you can try yolov8s.pt or yolov8m.pt for better accuracy\n",
        "\n",
        "model.train(\n",
        "    data='/content/drive/MyDrive/Tv/helmetdet/data.yaml',\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=16\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AK98_Xq4_ZXD",
        "outputId": "85971500-dcec-48b7-f692-20af87830803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.25M/6.25M [00:00<00:00, 116MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.121 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/drive/MyDrive/Tv/helmetdet/data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, cutmix=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 24.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model summary: 129 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 76.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.8Â±0.4 ms, read: 0.0Â±0.0 MB/s, size: 22.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Tv/helmetdet/train/labels... 1070 images, 8 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1070/1070 [07:57<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/Tv/helmetdet/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.7Â±0.2 ms, read: 0.1Â±0.0 MB/s, size: 20.9 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Tv/helmetdet/valid/labels... 309 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 309/309 [02:10<00:00,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/Tv/helmetdet/valid/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/50      2.02G       1.52      2.756      1.283         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:24<00:00,  2.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.823      0.067      0.399      0.228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/50      2.53G      1.484      1.804      1.207         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530       0.69      0.654       0.67      0.369\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/50      2.55G      1.484      1.664      1.235         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.727      0.598      0.643      0.372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/50      2.57G      1.451      1.511      1.228         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.707      0.636      0.693      0.392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/50      2.58G      1.485      1.379      1.241         70        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:22<00:00,  3.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.677      0.652      0.694      0.378\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/50       2.6G      1.472      1.321      1.245         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.698      0.739      0.734      0.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/50      2.62G      1.461      1.244      1.226         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.737      0.711      0.754      0.439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/50      2.63G       1.44      1.157      1.216         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.805      0.753      0.799      0.462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/50      2.65G      1.423      1.102      1.215         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.753      0.768      0.774      0.446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/50      2.67G      1.452      1.139       1.22         58        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.802      0.736      0.798      0.452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/50      2.69G      1.403      1.049      1.178         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.792      0.765      0.807      0.463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/50       2.7G       1.37     0.9997       1.18         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.797      0.743      0.804      0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/50      2.72G      1.427      1.055      1.212         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.736      0.802      0.801      0.461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/50      2.74G      1.395      1.006      1.194         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.807      0.744      0.808      0.473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/50      2.75G      1.364     0.9875      1.165         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.784      0.787      0.811      0.463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/50      2.77G      1.343      0.933      1.164         48        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.783      0.756      0.813       0.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/50      2.79G       1.35     0.9199      1.161         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.784      0.775      0.808      0.464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/50       2.8G      1.335     0.8982      1.166         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.789      0.796      0.831      0.483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/50      2.82G      1.318      0.872      1.154         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.819      0.771       0.84      0.499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/50      2.84G       1.31     0.8496      1.144         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.787      0.819       0.82      0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/50      2.85G      1.304     0.8633      1.145         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.819      0.799      0.841      0.501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/50      2.87G      1.331     0.8543      1.169         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.776      0.796      0.819      0.491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/50      2.89G      1.269     0.8357      1.133         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.787      0.754      0.808      0.479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/50      2.91G      1.295     0.8316       1.15         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.787      0.782      0.807       0.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/50      2.92G      1.255     0.7942      1.133         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.814      0.761      0.819      0.493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/50      2.94G      1.273     0.7788      1.132         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.776      0.795      0.826      0.499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/50      2.96G       1.26     0.7968      1.124         57        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.818      0.811      0.828      0.483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/50      2.97G       1.24     0.7584      1.121         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530       0.83      0.763      0.838        0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/50      2.99G       1.22     0.7628      1.118         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.791      0.821      0.842      0.502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/50      3.01G      1.237     0.7591      1.116         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.813      0.801      0.839      0.497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      31/50      3.03G      1.225     0.7612      1.119         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.822      0.787      0.839      0.496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      32/50      3.04G      1.217     0.7554      1.119         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.815       0.78       0.84      0.505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      33/50      3.06G      1.204     0.7313      1.101         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.823       0.82      0.851      0.507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      34/50      3.08G      1.216     0.7236      1.104         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.827      0.799      0.831      0.489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      35/50       3.1G      1.202     0.7176      1.108         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.839      0.817      0.858       0.51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      36/50      3.11G      1.186     0.6892      1.088         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.778      0.828      0.841      0.505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      37/50      3.13G      1.141     0.6601      1.091         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.827      0.803      0.849      0.509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      38/50      3.14G      1.152     0.6754       1.08         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.826      0.833      0.854      0.514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      39/50      3.16G      1.195       0.69      1.091         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.856      0.796      0.853      0.519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      40/50      3.18G      1.154     0.6684      1.091         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.849        0.8      0.848      0.514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      41/50       3.2G      1.125     0.6152      1.091         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.837      0.823      0.849      0.508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      42/50      3.21G      1.101     0.5922      1.073         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:18<00:00,  3.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.827      0.835      0.854      0.506\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      43/50      3.23G      1.069     0.5818       1.06         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.813      0.821      0.825      0.495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      44/50      3.25G      1.081     0.5685      1.064         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.807      0.842      0.854      0.508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      45/50      3.26G      1.056     0.5536      1.057         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.796      0.846      0.851      0.512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      46/50      3.29G      1.045     0.5537      1.051         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.824      0.831      0.842       0.51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      47/50       3.3G      1.041     0.5389       1.05         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:18<00:00,  3.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.826      0.834      0.851       0.51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      48/50      3.32G      1.025     0.5359      1.044         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.832      0.808      0.846      0.507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      49/50      3.33G      1.005     0.5129      1.034         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.816      0.833      0.855      0.515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      50/50      3.35G      1.005     0.5134      1.029         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:18<00:00,  3.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530       0.81      0.839      0.849      0.509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "50 epochs completed in 0.337 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.121 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        309        530      0.858      0.794      0.853      0.519\n",
            "           With Helmet        101        196      0.878      0.781      0.884      0.566\n",
            "        Without Helmet         64        163      0.737      0.706      0.718      0.349\n",
            "               licence        159        171      0.957      0.895      0.955      0.642\n",
            "Speed: 0.2ms preprocess, 2.3ms inference, 0.0ms loss, 3.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0, 1, 2])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7d75acf9cf10>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,     0.01393,   0.0069651,           0],\n",
              "       [          1,           1,           1, ...,   0.0021079,    0.001054,           0],\n",
              "       [          1,           1,           1, ...,    0.024268,    0.012134,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.34926,     0.34926,     0.44678, ...,           0,           0,           0],\n",
              "       [    0.14306,     0.14306,      0.1952, ...,           0,           0,           0],\n",
              "       [    0.24798,     0.24798,     0.33529, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[      0.213,       0.213,     0.29169, ...,           1,           1,           1],\n",
              "       [   0.077515,    0.077515,      0.1091, ...,           1,           1,           1],\n",
              "       [    0.14178,     0.14178,     0.20189, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.96939,     0.96939,     0.95408, ...,           0,           0,           0],\n",
              "       [    0.92638,     0.92638,     0.92638, ...,           0,           0,           0],\n",
              "       [     0.9883,      0.9883,      0.9883, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.5525063250485964)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.56596,     0.34942,     0.64207])\n",
              "names: {0: 'With Helmet', 1: 'Without Helmet', 2: 'licence'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.8576898482694874), 'metrics/recall(B)': np.float64(0.7936235197986202), 'metrics/mAP50(B)': np.float64(0.8526982965314308), 'metrics/mAP50-95(B)': np.float64(0.5191516615505037), 'fitness': np.float64(0.5525063250485964)}\n",
              "save_dir: PosixPath('runs/detect/train')\n",
              "speed: {'preprocess': 0.19978566666825365, 'inference': 2.346613488673535, 'loss': 0.0010478317156774263, 'postprocess': 3.0615026893190467}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "YE_fD640_aJk",
        "outputId": "89c56b68-3350-4f20-c93f-67948da9575a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eb234f91-f55e-455b-867f-2dd134bb5e97\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eb234f91-f55e-455b-867f-2dd134bb5e97\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving no helmet.mp4 to no helmet.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Upload video\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Load the YOLO model trained with 'helmet' and 'no-helmet'\n",
        "model = YOLO('/content/runs/detect/train/weights/best.pt')  # ensure this matches your actual path\n",
        "\n",
        "# Open the video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Output video setup\n",
        "output_path = \"output_helmet_detection.mp4\"\n",
        "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "# Process video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run detection\n",
        "    results = model(frame, conf=0.4, verbose=False)[0]\n",
        "\n",
        "    for box in results.boxes:\n",
        "        cls_id = int(box.cls.item())\n",
        "        label = model.names[cls_id].lower()  # 'helmet' or 'no-helmet'\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "\n",
        "        # Set color: green for helmet, red for no-helmet\n",
        "        color = (0, 255, 0) if label == \"helmet\" else (0, 0, 255)\n",
        "\n",
        "        # Draw bounding box and label\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "\n",
        "    # Write the frame to output video\n",
        "    out.write(frame)\n",
        "\n",
        "# Release everything\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Trigger download\n",
        "files.download(output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "collapsed": true,
        "id": "zeROM0f2_dPy",
        "outputId": "b11a28d8-c79f-4d0e-fe44-a0957adbfff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ca32819c-d858-4bc1-b4db-117e39d8cc0b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ca32819c-d858-4bc1-b4db-117e39d8cc0b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving no helmet.mp4 to no helmet (5).mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9eaabadf-db24-4959-9620-1e4ec28feaf8\", \"output_helmet_detection.mp4\", 13946737)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IsddAXVuQHa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tripple Ride Detection**"
      ],
      "metadata": {
        "id": "0BEvcgSJ_6Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxz5VZ99_hIJ",
        "outputId": "7a0316ec-ab4a-4027-b930-13ab0c68a9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "helmet_path = '/content/drive/MyDrive/Tv/helmetdet'\n",
        "triple_path = '/content/drive/MyDrive/Tv/tr'\n",
        "multi_path = '/content/drive/MyDrive/multi_violation_dataset'\n",
        "\n",
        "os.makedirs(f'{multi_path}/images/train', exist_ok=True)\n",
        "os.makedirs(f'{multi_path}/images/val', exist_ok=True)\n",
        "os.makedirs(f'{multi_path}/labels/train', exist_ok=True)\n",
        "os.makedirs(f'{multi_path}/labels/val', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Mf-YqFuhAHzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"$helmet_path/train/images/.\" \"$multi_path/images/train/\"\n",
        "!cp -r \"$helmet_path/valid/images/.\" \"$multi_path/images/val/\"\n",
        "!cp -r \"$helmet_path/train/labels/.\" \"$multi_path/labels/train/\"\n",
        "!cp -r \"$helmet_path/valid/labels/.\" \"$multi_path/labels/val/\"\n"
      ],
      "metadata": {
        "id": "Cn0W7L-HAMSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relabel_triple_riding(old_label_folder, new_label_folder):\n",
        "    for fname in os.listdir(old_label_folder):\n",
        "        with open(os.path.join(old_label_folder, fname), 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        new_lines = []\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            parts[0] = '1'  # change class 0 â†’ 1 for triple-riding\n",
        "            new_lines.append(' '.join(parts) + '\\n')\n",
        "\n",
        "        with open(os.path.join(new_label_folder, fname), 'w') as f:\n",
        "            f.writelines(new_lines)\n",
        "\n",
        "# images\n",
        "!cp -r \"$triple_path/train/images/.\" \"$multi_path/images/train/\"\n",
        "!cp -r \"$triple_path/valid/images/.\" \"$multi_path/images/valid/\"\n",
        "\n",
        "# labels (after relabeling)\n",
        "relabel_triple_riding(f'{triple_path}/train/labels', f'{multi_path}/labels/train')\n",
        "relabel_triple_riding(f'{triple_path}/valid/labels', f'{multi_path}/labels/val')\n"
      ],
      "metadata": {
        "id": "ksvBIpmTAOg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_content = \"\"\"\n",
        "path: /content/drive/MyDrive/multi_violation_dataset\n",
        "train: images/train\n",
        "val: images/val\n",
        "\n",
        "names:\n",
        "  0: no-helmet\n",
        "  1: triple-riding\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/drive/MyDrive/multi_violation_dataset/data.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)\n"
      ],
      "metadata": {
        "id": "ZfzYosCDARt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')  # or yolov8s.pt\n",
        "\n",
        "model.train(\n",
        "    data='/content/drive/MyDrive/multi_violation_dataset/data.yaml',\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=16\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "02bHKXy_AUNv",
        "outputId": "acae65e8-050c-4060-8383-1e925b641ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.121)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Ultralytics 8.3.121 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/drive/MyDrive/multi_violation_dataset/data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, cutmix=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.4Â±0.1 ms, read: 14.3Â±9.5 MB/s, size: 24.3 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/multi_violation_dataset/labels/train... 1389 images, 137 backgrounds, 550 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1389/1389 [00:17<00:00, 79.58it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-11-_jpeg.rf.9b5f25ce3c8b6a3e09e479cc5fe3d42f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-11-_jpg.rf.1691b345213c4d586fce8251d6884701.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-12-_jpeg.rf.ec25c750d49bf476d01cbe34b9ce80ed.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-12-_jpg.rf.d346a9c5f25db6563fdfef203b133ca5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-13-_jpeg.rf.3ed9dbdd5d7d06b3d8c8476fe9d28d14.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-13-_png.rf.bdcfb28d42b1aa18b93adf5eb96f813a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-16-_jpg.rf.b8fe5575459f1772c3f43eb7ccf53e21.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-16-_png.rf.3fce3d6302de035a101be79dabde24c5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-17-_png.rf.f13fe6a64e30562642717ea22d0f3dc6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-2-_jpg.rf.ad5dabf7173f722bf472ba33691e0d2f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-2-_png.rf.de70690551c36c81598186146548b2c8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-20-_png.rf.5df4b9b47596c27553a7b5f543e3f1e2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-22-_jpg.rf.1ee6998de41f7a8fa47fe5b750c9d9de.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-22-_png.rf.901fff869376365474abc1db5edf1705.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-23-_jpg.rf.a456d991eaef36b731256c50a8b525fe.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-24-_jpg.rf.1c6bf73b8c49ea79a8223f23c59f08ce.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-29-_jpg.rf.07a5ef1d07e160d567b2a147ea8b2c54.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-3-_jpg.rf.13c8404ef5270ce1150c08e4a7de04c1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-4-_jpg.rf.5a7418eeb07e789021f45e781abdf739.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-42-_jpg.rf.6ee5a78ab4fb2a09459fbfd84d3a19a4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-59-_jpg.rf.d7e49357a8c286e5917afd5fd6f3aa9e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-64-_jpg.rf.140bf31992c92f4411f3d5636fb67db6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/1-8-_jpeg.rf.a7791e26b4d9fba41d4f36f2fe7954f1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/CarLongPlateGen1218_jpg.rf.dbb7b65d4ee113192816850eaae9e589.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/CarLongPlateGen1478_jpg.rf.ff148b8232edd3c477722a3e1d270429.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars100_png.rf.fcd3ebfa2cbf7dc7a80a8628ef7c9447.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars101_png.rf.159adbc9e4df231d4a496971336e869c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars103_png.rf.7f70052c345c8a85d096f1258dd5ce53.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars104_png.rf.e66c3cb0ffe9545c0c8742589bf82016.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars105_png.rf.11af30d8cea8953c698972e25f42e530.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars106_png.rf.acf8092f11cd8454e2999591b242555f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars10_png.rf.23eb0ed9d0eef25c5e1cac9542e0221a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars110_png.rf.3d9b324f9febb003dc6c81d929553d6b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars111_png.rf.01015279f5c13860c67e9606d06fcf12.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars114_png.rf.bab41b2c2765ea6ade754efed74e2843.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars115_png.rf.df504d371a8ae929082d6e99f372049a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars117_png.rf.78f585a34f2581fec55855e5195e9a84.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars118_png.rf.a190a21cd93117f5b5c3ad88d2f50f50.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars119_png.rf.fc03a191c973ff39ff439a5ae46cf947.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars11_png.rf.d262b6424e048fc67f5b0f6c03e8bc96.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars121_png.rf.16158992b2e0b44877db27280fc1eed3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars122_png.rf.023f9fd967d0f9b9f04dedecc9d06b4e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars123_png.rf.8a5f511081588adf6d2869c433422ea6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars125_png.rf.36279625481bee6c1c6d938eba23e72a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars126_png.rf.2168857fafd68e5281e3d07589ffbd32.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars127_png.rf.e2a0d28a7adca167e127237b234d5d1c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars128_png.rf.415362de88b491efa36584c1dcb4dbaf.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars129_png.rf.23f48bf1f6e56a2e6dfe3d7e34d2e485.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars12_png.rf.226a08bf477abd9546972c252cdabbda.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars130_png.rf.4569aa6bece6567ea8fa466e064a8da9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars132_png.rf.ecc66c6e0b3de8fdc9343d6bd72b6b3e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars134_png.rf.94a9fe9b7ffacf1ed35de618ae269869.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars135_png.rf.516dddafadc88d8bcbad490b4d58802f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars136_png.rf.cf6f95c1136c0a5921bc58439f53476a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars137_png.rf.c0f1dba44d6df23730f49e6ca96aaf62.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars138_png.rf.246fd43aa8b9035e41837717a591d226.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars139_png.rf.b60ee2fdfac66ee0c6249049cf28debe.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars13_png.rf.75c7975cae70deab47f87f5e4c42abe1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars140_png.rf.c76d98518511085e67b4b36b0384193d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars141_png.rf.4444bb9209eaa37ca76c069cd95b8992.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars142_png.rf.7b1716c9add66000348be6bdaed3361e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars143_png.rf.b4bcb3d754fe764dd88b4622a05dfcf8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars144_png.rf.94d7c9f2d04245a4f8722e9207f095d5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars149_png.rf.6f88b6089dad4ef66d9c840f1e967cd6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars151_png.rf.d88fd2b3bc7f59fccaa8986f97cfbe74.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars152_png.rf.9bcf58fc9b55d41cd71f77446bcf6977.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars154_png.rf.5eb4cc3c8607e541bd14ee3d62ff79c7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars155_png.rf.6aa2cd2684b60dfd9b7f255f97119d27.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars156_png.rf.7630af4537e0013401bf0fc104921d93.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars157_png.rf.e894cee7cddd8e10d0156c45f819e19f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars158_png.rf.a8d661ff415d3af4c62043348d55ebc2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars159_png.rf.cb3779789404bec773f109951bbca388.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars160_png.rf.5f4c2ad9e2462dc24a3deb7e49be108c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars162_png.rf.2bd20514a28ec7f666ef8a5fc2555e83.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars164_png.rf.6b0fc4990c1dd688a5b16f0961b1ab0d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars165_png.rf.8239f5c66a947d9103c8e6b6626e454d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars166_png.rf.de97a7cbb290e436a5fbd31f590505b2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars168_png.rf.6ea3ffb2eace664b43df8b447d9e075b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars16_png.rf.abd61745f6fc6fcce35e805b9f741b13.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars170_png.rf.386bca669e38837c6a3d76ce76be165f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars174_png.rf.01fb95fc6113780da7110974ad7ed82f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars176_png.rf.01c75c37d39dd5ffc1b03ba6042cde74.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars177_png.rf.d17693abcde3d835e5fdb8a7d70f43e5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars178_png.rf.6b9de3eb4f6809a5fed798b7301232c2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars181_png.rf.d3353fc9e29928b58e11f6864e4461d9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars182_png.rf.9190f65c14a7c2a4a097709ffc019b45.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars184_png.rf.0349e2c50b9a969b27ee4ec7727b2c55.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars186_png.rf.bf7b79bae5537222c85a885b900266a3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars188_png.rf.268d10dee0cc629f930b675d4c4a86d8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars190_png.rf.6a4d1b2746fafa2b627c9bafe114f6e8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars191_png.rf.237182e61bc6bb30980c16b60d9e1b66.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars193_png.rf.6e907ace8ac0cf8f298e6867b7a9ea17.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars195_png.rf.302279aee827e11dd9114d0a3259cfe2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars199_png.rf.17a3990e704a10bbcc3a931e0b798011.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars19_png.rf.4b69a13a5286864ad05324fcf23e5de4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars205_png.rf.1202c4ff1be9a80bb66875b5daff0571.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars206_png.rf.426c025ff4377b3e426af459adaf781d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars208_png.rf.fc94f13c84d7380e0b9aad5d9f6d7b1a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars209_png.rf.eeeafd1e60b9b774ac62814f82e31a31.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars211_png.rf.a1027c84a2e22eaf55cbb2a8f2ed6e8d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars212_png.rf.6a9397934ac9bd299d3cd9c725511e65.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars213_png.rf.0e462555b82f1d5f9568e3aad2811a96.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars214_png.rf.761656b5fe33e632cca7adedc2ce858d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars215_png.rf.08d9b32300f0bc13fac2ca4324f3a03f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars218_png.rf.3dea88d2689374a239a788a443bb3137.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars219_png.rf.4c96625429435d650ec4b502ca770ca9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars220_png.rf.cfd7929e25df17ebe7e7dd9f26ada6c9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars221_png.rf.336b947e60e5e9a8c8ff2fe0a7357e9d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars223_png.rf.a46b16a142d2d55f63f3ee5c237a7f8b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars224_png.rf.0a08f91d4152235133d3cb9f7ecbe93f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars226_png.rf.edc523ac3468e8f059d80ebde69524f4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars227_png.rf.8d6e2946c7449cf1b47992bd6e401d8a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars229_png.rf.adee3a446b66ee01b469230568d55b58.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars231_png.rf.69a068b631eddb3e9d50f27dac513234.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars232_png.rf.d5cd7b3008952309710ce717ae9b8624.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars234_png.rf.f308f73c30de6cdf2dbc9392b5a9c36e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars235_png.rf.e137f76cc072da5901c0c8a999ed84db.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars236_png.rf.a5b5b557759639d9a24618cce0668797.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars23_png.rf.36dc2aff5eba2d8a9243535fcdedbd90.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars240_png.rf.145d108fc1b8da18d35494a2b7f96792.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars241_png.rf.6e65dc47df8538090d7238f3bd632bd2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars243_png.rf.60030b41dec97bf40d8bb1db438cf298.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars244_png.rf.05238696ec48ec50ffe5b47b6c19ad44.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars248_png.rf.2ce50a5573a386bd0edd63ff6fd4fadf.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars251_png.rf.85e1b4eb3d98bf5450bd9ffd04c30f06.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars252_png.rf.661d4e988a5578735e761804eaeff68f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars255_png.rf.c38b91d725bf4a887421056d54eff540.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars257_png.rf.c5b3d3364c2c5c457c52004317e42528.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars258_png.rf.d0e41570be93def993031c97488a9963.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars260_png.rf.5d9fb18efafa77b33faf160d1caf4001.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars261_png.rf.8bfb87022bbec1a7d85cf5cbb06ad4da.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars262_png.rf.a75ebb044a88bd3d1299d57a687ce8ac.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars263_png.rf.ea59c9954af15221cfc66c7c7f694ba0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars264_png.rf.0a02e7895de665a341b06a649a303d8e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars265_png.rf.40f195f2f7758bf931af69a391600417.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars267_png.rf.29cebd7076e66e196f31966f54d2d28e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars268_png.rf.15d85c8cf8d3e0324590790c39bec8bf.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars269_png.rf.35e36a70c3b4314b7158db986701fc4f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars270_png.rf.72d7ca964612954346f8584d861e22bf.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars271_png.rf.a61d161111ffad94c2bb07b7269339d0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars272_png.rf.04668a7b5fdfc2a5590e4f8c2960cbd9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars275_png.rf.89348e2dcc62ae7ae20760406253e5aa.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars278_png.rf.bfd20826989db4a33b99898d0a9fcced.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars279_png.rf.4a3fe5ee28ffe42d956e31bd38364d61.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars27_png.rf.d4bca5c8927b21c1d05ba204b417b778.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars280_png.rf.2b3e0231942ebcd8a4e9ddf8eb75f9d6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars281_png.rf.748feac00a66ed538fdffce0acf0fce9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars283_png.rf.54f0247b88a616c3d25be8fa1a6eb766.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars289_png.rf.12fe5daa3a9c0abe7e9a6fc0405fae7c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars291_png.rf.300c6cd46c33773cc9f38d06e1f54a96.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars292_png.rf.b65ac57a499e5283b6fd3da9887d7cc6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars294_png.rf.a378b3dc3f9d73b025ef1d0495481ac1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars295_png.rf.a0e56997e7342a95671f3e11d3cc8146.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars296_png.rf.54ae5c8623419db812d3cd0aaec1f6bd.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars299_png.rf.b5546dd99e55e1b2b3c72c475a84546f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars2_png.rf.e867224d14c690f7af40abc88606c8eb.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars305_png.rf.46fd1012e68a2f6346fd91a114707d12.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars306_png.rf.50640be96169bbc6f834b44b1af439bc.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars307_png.rf.b457f678918e30095a8f091d2bd88af0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars309_png.rf.5e9508d8244141c34fb2b616c6374871.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars312_png.rf.84fca02cc2033cb7bfe7eb1d6c497786.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars313_png.rf.761d4a3dd3f61c70c5155a9194360712.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars315_png.rf.8e4fc399923df5b2d25c08439e28206b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars316_png.rf.f5670839f5fb1387635425cb22789401.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars321_png.rf.a63b4ad50250cabff0b377f676a80b31.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars323_png.rf.6818afb1fb7d1e92a6620ea01ac8e26b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars329_png.rf.b18947c801b7450a0baaf908e612d819.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars334_png.rf.4368708445e7d3fe473a6c09a5e2e2e1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars340_png.rf.83c78f5f79e87e6be15fa0251054b7ef.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars343_png.rf.a6808748898b95f41582a8b489320b6d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars345_png.rf.0e608240da35e822eb3440cf39cdf662.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars347_png.rf.f6f264b4a1d2f456a25d1c38123a9c38.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars350_png.rf.92139f3d643ff557981adf197d39c1f0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars351_png.rf.cb99c04cba84584768638c8c753f904b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars352_png.rf.eac94cd011860ce0b7596f225785d5da.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars354_png.rf.72e6f11a5283ae294ebfa8870b1806a6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars355_png.rf.9356819537b2f6ebdc00e80399306234.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars356_png.rf.bbee7cded43346568215bd9176727dec.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars359_png.rf.79e8fcc1a57aed2c06b6f06c313ac78f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars360_png.rf.5286a6129899282386173edb569f71d4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars362_png.rf.7382f9d111bdabff662aedd973740486.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars371_png.rf.8e09911245bef36b558fcfdb79b30886.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars372_png.rf.49c3a9dd1422b1e746d85e9e84f486e0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars373_png.rf.bdd3ab5a40a7c7dbe94289b9cfc21171.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars375_png.rf.123662d7c20af6e32fadab702fde48ca.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars37_png.rf.a1b93ae7a96b2c6540f282bfcf4a693f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars380_png.rf.6bbf33d41a32dd8d25d628b7081ff614.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars381_png.rf.bc9547898e13b468e75ff1d533087c9e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars382_png.rf.4c99442ca4608e5d6f3c514130d11530.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars383_png.rf.9c4984c5f310f390220c1153db908db0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars386_png.rf.e3bedb4bee8745b6bfe84d5108a9d674.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars387_png.rf.b36bc31f1fbd7fad141441357c5fe613.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars390_png.rf.f8b4104ee20808ebcc0a1a86b8a7d657.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars394_png.rf.d27204a0c1ad021c19dc996d17aa66d5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars396_png.rf.d755a037034e07ab0e672ba4f5481cb4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars39_png.rf.3281ab08599c061e42356f219ed3e91e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars403_png.rf.32319d9cca30c732a15a36e91bd7bf01.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars404_png.rf.74da87449d383624e6dd50d35780b16d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars408_png.rf.015d173eee89632d34a5f0d9e1570619.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars410_png.rf.6b203c4e31f6af37a7bbb1fdcf9bf95c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars411_png.rf.fb4ccb5bda624bcfad5b93b29999f3c1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars413_png.rf.08d281bedb55ddd742f639decec1d127.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars415_png.rf.be5685d16696bad8fb3650cc088d9f25.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars41_png.rf.e8ff6888167d5dc74a29abd9e51fb7fb.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars421_png.rf.b38bd8ebe81e6caf32910a5f1a9c5f0a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars423_png.rf.3790302f951171f68dad77fea97b982d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars427_png.rf.41a36539d992d2fe5c9535ea83dfa45b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars429_png.rf.80542c767f43724aaab2f65006b17e64.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars430_png.rf.1d0257599a122a4504dafe85a1b0f1a6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars431_png.rf.21190c64920d9c1a52eaf4e3b4d41a35.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars44_png.rf.e1f591073c3e57ae1a7c74e6d7cf1d2b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars53_png.rf.7aaf995d7eeb5a96427c3779d3da8605.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars54_png.rf.79c50bd70e31f65115f042ef84d33c3e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars59_png.rf.6b550f2b5f2602911c6b216a13422647.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars60_png.rf.854e18d747f096cdd0734209ed7c9e9e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars62_png.rf.67bdf6191397dafd2cfaaf7b2e15d8d3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars63_png.rf.f7b6b4fe20f028d3b306cf1d1c412923.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars67_png.rf.dd206427227a84764499d0d4a815e8f7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars69_png.rf.e90987d39c893c9b896f5aed9e0aef49.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars6_png.rf.c1b464e5221d0854fa2540e415c210f9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars70_png.rf.48372ac3c278ed6d0b4f7f6d4e7ff408.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars75_png.rf.802e24cd8a8845192170d9221df43640.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars77_png.rf.70dd8be78d4e40e484dd41e5fedcc22d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars7_png.rf.ca8d43b77c937cddc7539dcd85808a0c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars81_png.rf.dd72d00de7bc2dce8f5e48b13cf4e5f7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars85_png.rf.212d9ce3ad11417376818208187f8a1e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars87_png.rf.13b46cbc829061e8947bd91aad95eec6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars88_png.rf.b6dabca8a22feec87a2aa4ae33fd8fac.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars89_png.rf.c8af505830401effb3f11a148f7ad954.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/Cars98_png.rf.a33e5557f1aadf51b477343dcca8f6da.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/brightnessquandoi88_jpg.rf.a4a77861d328b20295879599fa4770b1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1000_jpg.rf.4e41914f797c9037f183752a61e54193.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1001_jpg.rf.6e871efdd97b36ecca88253353b556a1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay102_jpg.rf.f22a3e0bc14e602a3552c6168718efb4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay105_jpg.rf.2db126089567ae9deb1d3731bd56cb83.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1063_jpg.rf.cd36ba95954bb9585bee1547e2054e46.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1064_jpg.rf.0b309a46d77dbfb1529a122bc04c65b7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay106_jpg.rf.f9350c554f3c4926cec7c213dac28e90.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay107_jpg.rf.3ef715b5e6b25676528d013a4dd40f49.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1082_jpg.rf.416c53e3cb76f992dd778639add47fbc.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1087_jpg.rf.30ed8f750d16e838296e1648e774a3c5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay108_jpg.rf.7ec6e1c43fae330507085db2e6ba00a3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1107_jpg.rf.fc2bc1de488bd840926942ac8f1c5847.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1108_jpg.rf.c913a0dae2d71e8cce897691509de016.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay110_jpg.rf.b88a874b9f4060465e0889c5019ed772.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1138_jpg.rf.3cb5a66e880a531d54508ec4e357efba.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay113_jpg.rf.6a9db037c9cda27ea39769d5faf26ea6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay114_jpg.rf.25b175f3db9d00165928f24f3cd9175a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1152_jpg.rf.2cd8e9bf484d4b5985b2f392e2c0f921.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1154_jpg.rf.1cf2af12ccb3c04e046fc14a33920232.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1158_jpg.rf.5b1d1b145eb1dae89ce0dd4c20cd4b19.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay115_jpg.rf.e847c017d9f1309aa88b7d9e7671f084.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay116_jpg.rf.c76d4de9194bcf8df03e8bff0f8d7e6a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay117_jpg.rf.7f7bb5da7571b3c5dd30a97d96be7f79.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1184_jpg.rf.51ce0e5776ea2966aedaa4fd0b8cecdd.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay118_jpg.rf.9fd671f3ee6e7ae3cc1c99201481eac0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1191_jpg.rf.924e0cdd62c8c143ed2d107329fafcb5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay11_jpg.rf.e62c333a4e62b16c95b0c09bf3e46608.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1219_jpg.rf.a54c20632eda85cd9435dc7301e6534c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1223_jpg.rf.5b9470cbd2f06d58e50269c05ec63653.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1231_jpg.rf.78c1cefdfab92ee06218e9ed29eff2bc.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay123_jpg.rf.98986791f2be7b1d4c0516ce796b509e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1242_jpg.rf.516a857e73b2d9f8b12203708c675979.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1258_jpg.rf.f84b8d8d001519e8e4ed951607a84e5d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay125_jpg.rf.dcaf656c510df650717650bfb83b40ca.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1266_jpg.rf.020e5a3ca8333426e321d7c7a055ef49.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay126_jpg.rf.fadd160504a6b8527ea389f72d12ef6a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1278_jpg.rf.c321a05805ca1e74be26e4999c04bb58.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay127_jpg.rf.a54afab4ee4a085a8ecda8c9cb94aae8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1281_jpg.rf.f1a5395ae4debddfebe7d55427327932.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay128_jpg.rf.b66fbf523eba092d56bc66597d464e40.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay129_jpg.rf.3a029ab182d751fccfff940d491d0126.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1301_jpg.rf.44d3f344a6b197f913708928334c13a0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay130_jpg.rf.de4ab61a72f202ddee5ef6bdfb19e85f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1313_jpg.rf.30ac5127fe432f91ab6bd028c52f17fe.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay131_jpg.rf.04bbafbef6ba9fa0baeb6b63c857f239.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1321_jpg.rf.4b6ea82737d84e4f1cf4e95df1a04e41.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay132_jpg.rf.1be18d4e181931b67f27374671fd741a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay133_jpg.rf.8574dc25c4e6989f2b88ad44686b4760.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1340_jpg.rf.ecd34d780bf8278af084150afb45df80.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1343_jpg.rf.1a449239b14be457a523e492f828aa25.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay134_jpg.rf.76830d8967503390bdeff5a1c7fbdc00.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1367_jpg.rf.d6913f2937d13916b7a9cb7079f396c7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay136_jpg.rf.69ca131bcf1a5dac1ed0ad9f9c2e9962.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay138_jpg.rf.d967b3762a2fbbd24d8290fb68b66e74.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay139_jpg.rf.af22b36deb874aeec8a6078e4d82e077.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1408_jpg.rf.c601936e257ab2f9495b143263e8f07d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay140_jpg.rf.167bc9206233f6ac2f557e1f56b47cba.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay141_jpg.rf.6a027e1adc7357b82b0b0e2ba13077a6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay143_jpg.rf.4b7898e49af7d83e1bbffb150d56cb97.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1442_jpg.rf.c9ffa17645b8f56d62e0c1204e0f7a69.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1452_jpg.rf.d71f7aff413ba8f472a9b67d3e2ba476.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1459_jpg.rf.4ceceb06fb139358a18179b9e408e05c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1470_jpg.rf.71b654fa9271b984b9999f06dae785b4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1480_jpg.rf.4a97dfe184865e73f4103d390147f124.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1484_jpg.rf.9de8ff2796fc5ca032dd0e61b545dfd2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay148_jpg.rf.25c5a6bce037370f338592998d98f99a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1500_jpg.rf.b54b613356fa2014dce830980846775d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1501_jpg.rf.0688d9f9f236514e43bc945a98f11919.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1502_jpg.rf.d671bef6a1f46ba03927790bc3400794.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay150_jpg.rf.617716f67cc0abcf934a11ac3ffc8a28.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1510_jpg.rf.cc6df57d2fbe7bcee854a764d1c77fd5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1513_jpg.rf.46c4ccfbd5c9077e4214e54d480e5266.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1530_jpg.rf.22bab62467c08a2709b33b6e158e2e05.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1544_jpg.rf.37f09967a221e965320492662cb6559e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1553_jpg.rf.be674f22499ad4288828e07e073692c1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1557_jpg.rf.5a007536f1755413aa9940200f9a079c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay156_jpg.rf.2f22ae6f3a991aa29735583477547deb.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1572_jpg.rf.10a5e082be9c6c5995e212ce137ca5d8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1573_jpg.rf.3594ba61ed193aae079fdb0464003c83.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay157_jpg.rf.068060a057fb0ecf9fd6c5275c3f0d98.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1580_jpg.rf.2cd10b1090cccbf3739e9eb22cb5ad2c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1582_jpg.rf.848ed4474f254643dc26f5fe4efc46b8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay158_jpg.rf.40f796f84897992a1d8535e1f1c8c667.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay159_jpg.rf.dfff330562f9cb7b4c1a8785976ab406.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay15_jpg.rf.157365bf7a1e2104544557de8f01a0c1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1601_jpg.rf.abe01b3c2138763724c8445f55c3e334.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1603_jpg.rf.c0405ca11e092f68ddba021cf5a470c0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1605_jpg.rf.36e2f4a124638c103c428bf8fa643b46.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay160_jpg.rf.5d99ded226a241f25919bf89111398b5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay161_jpg.rf.7fd1b928526d3a1936525be7302081d8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1621_jpg.rf.0e8e61d94f55f19384543e441a0523e2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay163_jpg.rf.cfc24297e78952e2897e6937524d39e2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay165_jpg.rf.7a8ec71f26d83e13a62d4c3a989ba928.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay166_jpg.rf.32cf87a9f3f10d782ad32a4eef20c73d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1670_jpg.rf.4f459851466273c9ee2837c3369a6b03.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1673_jpg.rf.0722831b701def1ddf20609f0153003e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay167_jpg.rf.97a127da59b216ee05fca155c5c11bb7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1682_jpg.rf.1e3058b22abce28f682369b54d42638d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1684_jpg.rf.6ef54c0dc3375ff78e077a7c4d2846b8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1689_jpg.rf.1d3dcce77c9f8416b3e1f515678e2d10.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay168_jpg.rf.651cb8d25d147a76056a43a2381b63cd.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay16_jpg.rf.a91774bb2cba8e46958285f4b81cc344.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1724_jpg.rf.c71174b123e433e8f5bcdd4606b8dadc.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay172_jpg.rf.0a65d384c97b64f2ba063ecb9f5b1d4c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1730_jpg.rf.2e5ba52ef6a2a6c38873b0d946f7833a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay173_jpg.rf.dc52e480bb1d02920b301c1099d10ca3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1742_jpg.rf.c8d8535e668dc1791d81bef5821cf975.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay174_jpg.rf.24763940d901080878eaa6b84c229c76.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1750_jpg.rf.d992e08a27b08ca8bc8d36e2d0aaf81d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1758_jpg.rf.2d77af4298b56520166aa6cbd2954c1d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1763_jpg.rf.865bcda799f8a340d9098e57087e0eee.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay176_jpg.rf.59752f18220750250c2ce1b7f406f1d1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1787_jpg.rf.100c84ecdc4f04cb84e4afaf5c9ea575.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1791_jpg.rf.c73b2543ed5ab6649fb9ae47a6dae811.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1801_jpg.rf.6df4fad880e613cef620dfc179407dbf.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay180_jpg.rf.6c99351c548bb06c4d5a05a66fc0c8d6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1817_jpg.rf.a610a3ee9d1aecfdc90c25b31058d3e3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay182_jpg.rf.dbe40d2ee5993f06b9c2e9d4cb96dec9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1836_jpg.rf.e4cd3c692ed0ddd8123128dd825bf395.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay183_jpg.rf.f38bc5f6cf769bff07dc784accd692cd.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1842_jpg.rf.52fc51c1fe43206607b91ac0c2a769f7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay184_jpg.rf.8925357cb5ec1522b3ec06dc3c908bfc.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1853_jpg.rf.c1280f794f3b6b99e81fa494c0d3d39f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1868_jpg.rf.c6c52b6fc52af686caa3892469083621.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1878_jpg.rf.9f05fbff45f4bc45c680a7bec319a473.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay187_jpg.rf.dc7fae6023b69963ecec55e641e6fee7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1901_jpg.rf.9ebb1295910614039432e243dcf75c7a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1904_jpg.rf.355bbaad207e5db7c7e6e023d40b0332.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1915_jpg.rf.d196e87c45a727fb80ca19cacff50300.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay191_jpg.rf.d3cdd0c8438df14286ed1185e4567e9f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay192_jpg.rf.d5933fe53dfb02774f38e3254ccc0e98.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay193_jpg.rf.6cec80f14f1a4c730f8b9803c145cb9f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay194_jpg.rf.6c92710a2dff691de2dbf167c7af77e2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay195_jpg.rf.850fb5f2e0ed1c9ebb53ccc689f4abc7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1961_jpg.rf.6de7fa97c7ccd4bbf969bd0cf288170c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay196_jpg.rf.a2412ebbdc918b8a9927fa8f1165d713.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1982_jpg.rf.37de7bae873c55b0307a79ed9d86e8c6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay198_jpg.rf.cc20ae4ebf34262dbf44715a73645f39.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay1991_jpg.rf.380392982add2f8d55bb44f81fe8692b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay199_jpg.rf.b45df1a53a3fb3f66157e4a8a27d67ee.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2005_jpg.rf.ece2eb8d955a05503a57b02fbea1dbff.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2008_jpg.rf.ecfb08512b09e9e19c17dfc414c2ca32.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2009_jpg.rf.e907afed2f197afd6be020686b9a049a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay201_jpg.rf.acad753e544c75630acb9952d3814265.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2029_jpg.rf.d53be59fc9e2225d4cde813b8dfe019e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay202_jpg.rf.d4bb579dc988b2dc43024f8ac1963678.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay204_jpg.rf.4dc51e39c31cb39a00dfa502759cb236.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2059_jpg.rf.cc423a875d4a2c3c08e261ccc05098b9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay208_jpg.rf.5614a509390c60055b6f184d135eb568.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay209_jpg.rf.be49ae123d6c61fb286de09ed3299b17.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2113_jpg.rf.dd43d6a390ab3548d173a70e624253d6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2121_jpg.rf.6d799ecdfbd04e818afab40f994b9f9c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay212_jpg.rf.d840f3accdc5a87c166e0bebb8833a22.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2139_jpg.rf.bcbb3ae08111bb284fa784c07e401891.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2155_jpg.rf.71686d3fd9625ae60d510b23d7817d17.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2158_jpg.rf.16257095e8259ba25b40303fb59afa96.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay215_jpg.rf.bef03eb0f59b03744383f8103a74b1b8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay216_jpg.rf.3051838cb236a6b164f73bfdf6b410fa.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2183_jpg.rf.4640a832c80c9a2a3bb3387a5670faea.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay218_jpg.rf.5e01f33ee4af420ae1e884912cfe78d6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2194_jpg.rf.b9a89b0fe10ecf2f27f4862cd106c76c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay219_jpg.rf.fd968e940bafce15732006ff6ae7cda5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay220_jpg.rf.b7162296408fc5ecfdf801362ad06f30.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2213_jpg.rf.15428b55c7df1c3d8397a8e0bb3340ba.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2214_jpg.rf.3c5d6192a6c971fc5ce0144caeb2d098.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay221_jpg.rf.d2a489b501920e8fda0053fa27d1b2d5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay224_jpg.rf.736497e0cd7af94c8366f982b30165c2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay225_jpg.rf.a946900aea845099f01a6a284aac2ca7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2260_jpg.rf.ed264d3539f1b2a7b74ec8c8dfe6e2b8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay226_jpg.rf.485873dd05714b350ce6698f637b69d4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2272_jpg.rf.b9ea28cb2403d62b94aadb22b171ff88.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay22_jpg.rf.f1390cbb4e0b0776de6fd99a258041f0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay230_jpg.rf.c018ec084a1bf1513811ed94d29e20c5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2310_jpg.rf.7a4b424772a510852a2b0a873abbd2c5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2316_jpg.rf.42d5b7e35becb8d67373f3ce09e4df13.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay231_jpg.rf.452004a6cefe3e43db2ce31d5fa52378.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2320_jpg.rf.2406ed7a80e16cd3c1c0e027523eeb31.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay232_jpg.rf.55fcefbfb422409872d44c00b42c8ed3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2331_jpg.rf.89610835c284200c853e680e8555b376.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2334_jpg.rf.142f60e198a2e9a6bbbd7857dbfbab79.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2339_jpg.rf.df909846fafd7203f1ef6a35eeec9634.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2348_jpg.rf.1a053c8c558ebc717a5d7a1beb41b8dc.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay234_jpg.rf.c1ecbc3966e642a1e3780a4e19c75708.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2365_jpg.rf.4867639f2749ef8da689d7ae6de42c54.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay237_jpg.rf.0432d2c61c69605a9516945f47106852.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2382_jpg.rf.e43e4795b0188b301484579c6d29a741.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2383_jpg.rf.1f11daf044b26cb3a8d40cc291700e4d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2397_jpg.rf.9da729da6bfbe80d3f841f1bc1e19187.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2404_jpg.rf.e5b3182261a1de39d290e498f2176fc5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2409_jpg.rf.dc892267530403f8147e668bc8bce956.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2420_jpg.rf.b1d52175184e4731c2122e089a09d56f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2426_jpg.rf.45557247750f047266aff070ecabed17.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2434_jpg.rf.8d7f79831c9e0bf71fbbeb0f55c91645.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2454_jpg.rf.b5952869b12dca3d9ee4de7c4c20ff27.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay2458_jpg.rf.00668b56e9b5b4db42171aead3e0e740.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay246_jpg.rf.9a9eebbd36c6f6809a589d0a3b494dd4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay274_jpg.rf.933855c73454a90970723d4215600548.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay276_jpg.rf.30effb76a6a64ff7df311b77d42bb192.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay281_jpg.rf.49ffd0f985e0952a8ab89079f1ceb652.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay284_jpg.rf.41250def4006315e45b687ea85f7b2e6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay290_jpg.rf.695192effa10a1472a3b98704c77ec52.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay299_jpg.rf.ee0fe2069608afddf60dad0b4e4e3eeb.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay29_jpg.rf.4de908031843269a512b1cb019e58cad.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay31_jpg.rf.29d771110ce1fd064ccd5ee0079c0f13.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay32_jpg.rf.fa3b5b3bdc0f8662523109c825866475.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay333_jpg.rf.49e5b2ba1d6280bc0f68b39730c624e1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay339_jpg.rf.f734dd64bc571c80aedc64a91ed1c274.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay340_jpg.rf.fb0601763ba8caa0a0a17e0763ec5659.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay341_jpg.rf.88144704a304a58c69985c6ca0d3f1b5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay342_jpg.rf.c7abdd667f890da1c356a79206da1ee7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay345_jpg.rf.0f80dd2ae4a6a4952c3493c0740bea74.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay356_jpg.rf.c358689c9a9ad1b971b40b9a14ae5684.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay35_jpg.rf.b191fd7bb2ad60b6e122e5e7f8dc63d8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay36_jpg.rf.0e269909356384cb0567b83f9efc7fb2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay378_jpg.rf.85969a560c099b86107d324ec0d08b1e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay37_jpg.rf.acb1538b32debfebca0ff08c8f5c6707.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay381_jpg.rf.d2f39ade8125a72ce9592a868f144f96.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay405_jpg.rf.3c0db433afe9887dfd156efe09e6b3da.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay416_jpg.rf.6ddf25177b7c8937a09ef5001a358cd7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay424_jpg.rf.0dd62a3dce095db058d7f5999e45453a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay431_jpg.rf.1dab8e374f9822d4c95e2258692e0e18.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay444_jpg.rf.df8fa948d9445403fc84a98bf45677b7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay44_jpg.rf.05667bb93bbe23dee1e355e2c7b0729c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay459_jpg.rf.683712b9cddd3af658fbc0905315e503.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay46_jpg.rf.524632771bf9cb03ac9ed3fbeefce5cb.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay472_jpg.rf.a98629b0edd4f16e757a9f7de3c2d863.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay477_jpg.rf.ccdd254c7ccfe7ff1abf0555f083fdb3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay47_jpg.rf.00af2cf947800fdb20f4f5d52fdc79ec.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay487_jpg.rf.09adb8721f7cd716e5eb45168470b336.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay48_jpg.rf.824cc80921dfacfe46ee02081accb1e6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay493_jpg.rf.1cad49092a2299d1670278079dbfa79e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay49_jpg.rf.ac0da5039891dbbc9dd0ea1ad6f7510a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay4_jpg.rf.96be3491f01faa701cadff40a8c52711.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay52_jpg.rf.1fc565651d07147cc4b641926d09546e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay532_jpg.rf.97a55e6db6a6d4da50f9626c993fdc40.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay53_jpg.rf.b4add8f3eb2d4949559106ef5421f7a8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay552_jpg.rf.999657c74faa1af96db6782b194d2679.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay55_jpg.rf.94c20cfc40c3d1db325a213109b42de4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay569_jpg.rf.1cdb8d50714266a391ee80cff0050c0c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay574_jpg.rf.c597aa18a44b79da932868f57aca2357.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay57_jpg.rf.5d60118ee22e2ae46f61eba37133e4ce.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay581_jpg.rf.63141adebca1e5adb08706981ed4f4d3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay596_jpg.rf.40cc622112ebbd3fe8f39d88ab70ebee.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay597_jpg.rf.5116c8515684a58aa373258fb41438e2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay598_jpg.rf.76f0d9e38c0b23aabca39c90aba11daa.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay59_jpg.rf.898833f2dd415cddf760d447bb82af5e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay5_jpg.rf.571471db5ad5e413732220209812817d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay602_jpg.rf.3ecd3dc33164f9dfd6ba9f90046c3093.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay608_jpg.rf.cd5b6fcf68aa8ffa45fe579dd95c02c2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay60_jpg.rf.8e00e1afca4666c5e9755d49aee5689d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay61_jpg.rf.cac15eacc68fae7c43f47f8b1cd74870.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay629_jpg.rf.a92f36d7f3287d0ae724a2907bca5873.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay62_jpg.rf.99401ff74b2011a6debb944183dd5da2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay632_jpg.rf.7aab2c94335de050d6f94a03e72c0184.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay641_jpg.rf.39fe774ffc3d4415bd8d343e15910a1f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay645_jpg.rf.92e54dac68675159eb918f6605ef960f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay647_jpg.rf.861d7ad7faae612d6716ca99c6b888d6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay64_jpg.rf.214dfc153638e943027266547559a1cf.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay65_jpg.rf.c7b2c8b072c53823f9f7df26f8c5b359.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay66_jpg.rf.487510279a3fe9aee9fc3d5e7a4f4a5e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay68_jpg.rf.8c8ce93f888b405ae69e4d64a25a23c2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay6_jpg.rf.8b6c95ccae70da7bd80f4c4f4afde1fe.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay709_jpg.rf.1c34b3675e3e06330471d7f83b351559.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay70_jpg.rf.f129306be361a7f77a58871c5ba76263.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay725_jpg.rf.2bbe393be0c301e33b1d103f66860892.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay73_jpg.rf.3002e5165cfddb92d9add874fa76b135.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay74_jpg.rf.78dfdd1a84357b92c75b3713b81fe023.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay764_jpg.rf.fe1b535468eea0b11cc2a9f6bc6f3577.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay769_jpg.rf.d54bbda81e8c88bdba0c06e4f39d87ee.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay76_jpg.rf.0a30b2e0ac8ae65633f5b610e1ac011e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay784_jpg.rf.b887888b76ae698869bac2e656132580.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay787_jpg.rf.69a7a5cd60b8c19516bab606e49983b0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay793_jpg.rf.ac6fdea4f9ea15d6acd2a0f531921a0e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay79_jpg.rf.0aca4afc77344d8db02af644d2e2bc1e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay7_jpg.rf.0dae4137aa55abb927a924e6e275c58e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay818_jpg.rf.f4524375221bdeb701921976cdd021c0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay822_jpg.rf.8093c5fbfec34828d9c1d49e828b1f44.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay857_jpg.rf.8199b44e79f0236cb2994d152caa2361.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay865_jpg.rf.8f8f03f439a37a7028cba08f73b0deb9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay867_jpg.rf.637fef428b31c11d2d4069a9376ab04d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay86_jpg.rf.454f4084480fcb23ceb1ed97113a7054.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay873_jpg.rf.552bfdfa05922b954f5f99d5c127dfdc.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay880_jpg.rf.c7e826d80289ec92b26b8d9801f1c6cb.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay88_jpg.rf.d9ee0a7914b05cd5f8b7ac64abea347e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay897_jpg.rf.4b0322e55c16a417e7937c45f2b4d2cd.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay89_jpg.rf.f7b1c6e35eccd0b0158e9ae039d7c3ab.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay8_jpg.rf.057ee357e2d90399636dd3787d8f03b9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay929_jpg.rf.9411497cfd29ac67c6c1b653ac17533f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay92_jpg.rf.ba4302a416bcfe1d2764997b7d2007a8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay938_jpg.rf.f504d31f3d5abc6310d8e9aedde46419.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay93_jpg.rf.4550bd1dad76e2c3251611b8aad78695.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay94_jpg.rf.3736c265e285a88be3fbe064e45f4be1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay957_jpg.rf.1110283c5664ca0595167f18bf954849.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay95_jpg.rf.c2c02e43838ec0aa4b17c1288a3eba54.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay966_jpg.rf.88ce0498d27989c32d6074fa243587da.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay96_jpg.rf.92ebae8752b4868601f97c27cd1f9cbd.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay98_jpg.rf.22c5c9ed0ae89671e7df617e0f8fc23f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemay99_jpg.rf.9c0f98a809dceb1eec80c600602eb57c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate102_jpg.rf.69d3c0f325171ff1da6f0b991ee39f81.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate10_jpg.rf.07241b8b768bd36fef37bb7eb6f582b6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate115_jpg.rf.3d83bb735df9c11ff5ab79d8c0399898.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate137_jpg.rf.46a149a7989c36fa578462f1ec6de43d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate160_jpg.rf.ba0910d4e5b7cc4839c05ef90ded18e8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate162_jpg.rf.715768dbdfda727e7f1e1614754c7533.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate167_jpg.rf.f4d0ed24c5fdf735eb510e9f5e2973d1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate181_jpg.rf.4cafeae138eb2e19666aa72409aad934.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate184_jpg.rf.63696ec81f434fbe144f3107a2d00b53.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate189_jpg.rf.de427e9f2e6f2411fbd1cc210207e109.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate18_jpg.rf.d469cee412465ea08e8d36f6c4c09398.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate197_jpg.rf.acbb8f613e95bfda1813e04b247265f4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate27_jpg.rf.7b943783195b298c827db44044facfa0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate28_jpg.rf.322e2c6f5a9800193fb5dddb4e0e6e66.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate29_jpg.rf.59934683cc68c1ff15bb960c8712a9ab.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate32_jpg.rf.1776359c43116b370bd0b7d80fe7907f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate33_jpg.rf.b9a8b0521287224452ff763001c4d182.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate54_jpg.rf.59ad92e5eb9e9c02e5200b14aba48fed.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate56_jpg.rf.9ff40c7ff05678d3cb6b640d33f88979.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate59_jpg.rf.c2ebcf3614f082206eb1b9f2fac77c77.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/train/xemayBigPlate9_jpg.rf.aa0b417940b6f2149f7bb23d10c49f6b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/multi_violation_dataset/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 1.0Â±0.7 ms, read: 6.9Â±6.1 MB/s, size: 20.9 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/multi_violation_dataset/labels/val... 309 images, 0 backgrounds, 159 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 309/309 [00:03<00:00, 100.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/1-14-_jpg.rf.d55a39d12069498054dca3861465591d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/1-31-_jpg.rf.6021306b751901b4e342e31f2c840385.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/1-6-_jpeg.rf.17ba5e519189287253b8d74ba2709e57.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/1-65-_jpg.rf.60ecf96afc8580d44ddb44951b6d607c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars0_png.rf.cff1523487a198cbe114ea57a25ba708.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars102_png.rf.d4c43e7d29731933202af5cf4f45915c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars107_png.rf.ed0a353e8ffed95f5e4604a9e56c7c11.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars108_png.rf.407bfd112f99e301386ed8be8ceea81a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars112_png.rf.14dfabf9ce66403b02708bd1aa38e714.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars113_png.rf.34045d94c84e95b1abd8466bcf3aeb5c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars120_png.rf.bf54113cba97d043518f10396ea35d96.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars124_png.rf.b19bfcf6e1b866ed6085a7041fc22c3f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars133_png.rf.9569cc6e097bc71dd6c6c10083ca0bf8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars14_png.rf.488acb9dab54cce21e55e7b9aae9fbd1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars15_png.rf.905374c6785f10f6780a7a894bc9e987.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars163_png.rf.f3c4a4209c2fcee9891cfb5636be336a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars167_png.rf.ea7a37e0515ed5d1a6f099d75dc1a234.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars172_png.rf.efd63c675d7425fe20b71a346d9211fd.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars173_png.rf.cbd52a18ddf6ea980de11ea9d6e8c58f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars175_png.rf.c9b3f5e2b8c3873bd2487570782399a9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars179_png.rf.5161f00040bb551064acd77449f40359.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars202_png.rf.63c5978332fb48335e5c405de06939a6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars204_png.rf.066dbea0682208d9b10546788b218bbc.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars20_png.rf.af700fd4eb46d253ea162010d3692cfb.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars21_png.rf.9ca7c8e3a1b191f04080b0764a1926bf.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars225_png.rf.71e51236e23de326d4b186c31f036c76.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars228_png.rf.0fa54638b2479920223b1e55c1b036b1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars22_png.rf.fe1651e245dfdbb3b787e6807b4192c3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars233_png.rf.e2b07618802b6ecf938d0ded041a2260.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars237_png.rf.5eb5d398c0c9aff7818f07f84e5c7f5f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars238_png.rf.4406c3052fb3fceebeb9973956ff832e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars246_png.rf.efba41595df1f3c4c657c85a8479225e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars249_png.rf.4c9c5df2d72400232169bbb9903eb9e4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars256_png.rf.44033c0133101e8c8b168c7ac983ba2b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars25_png.rf.66a53a202ce6e9510a9da62d5140e031.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars286_png.rf.2c3703c4f2ea7c75b2c71c361ceec75a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars290_png.rf.1dcf0600eeba1cd40b1d43876b82d39f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars302_png.rf.77d57bf35476208bdd865a6c5b89227b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars304_png.rf.bc23f4665382853c7be556590ddbebf5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars310_png.rf.ce25c23376a2d431034535f2a9402caa.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars314_png.rf.83d07ed48f4db39ff2ea14ae9ebe6db0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars322_png.rf.998aa537282891dab27040ed58fbb128.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars325_png.rf.7e64c0203af36e0bb350e8368645616c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars330_png.rf.fce7dcb66d8e24fd48f429c85cd8d60c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars331_png.rf.c12f1f1d85170153492870c9f4ee0c62.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars338_png.rf.ac6160f1cd6c2ef72729e771ec3af8f8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars344_png.rf.2cc391c2f70256ed0301b18fa66611b7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars349_png.rf.f8297ed9faf977a931fc32682e6c5fd4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars358_png.rf.84f9bd72ed7fa397baf9b004b6cac94f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars363_png.rf.39546d40750de59e9580a2c02566414c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars365_png.rf.881d29513cbd79c7277c4d3b9cce409c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars374_png.rf.55f575bc7a036577c871a6d5d6514472.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars379_png.rf.0b445a828fe996e1f977c56fb84402c3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars388_png.rf.d109e2a21a45f95ff546ef6a858d0730.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars391_png.rf.878c8edc3320f2810474083bd1f5632e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars407_png.rf.b0b356bd257904dcdc8a4f6f801147e9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars409_png.rf.75f3c9c38836abbef9acc0741d7895c2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars414_png.rf.da89db0fc67a083800609e9c76acfc04.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars416_png.rf.49ad87eda82b5636d3fea759e8f5da26.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars417_png.rf.407bf4cb9f5c360899e59608b482b441.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars422_png.rf.3fec7e7ae827529c6bfb40c9fdea87d2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars49_png.rf.c047e34a3ec6c1b2f9e34a9eb29bb6e5.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars5_png.rf.a8a23549eac9f3ead5746c5625493530.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars64_png.rf.021e8835f9ab72275a2cf2de68304659.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars71_png.rf.9578e91f33a167c6888a79cdce340680.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars76_png.rf.f38a842b445e2922c379145b7565ede1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars83_png.rf.df735067b735cb0d7fb9c06442ac2e66.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/Cars93_png.rf.d66ebf17c658ef99022a54c4cd0726e0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay101_jpg.rf.3eabe50279bc5c18971577e081a0e4d4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay103_jpg.rf.c516effd32552921413f26bd948c11e9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1040_jpg.rf.ec14e6657d7f8a9238da217fd34d12b7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay111_jpg.rf.191c19500b61742198877db3415ed288.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay112_jpg.rf.4cc96ea8cc6b328a38bf8894fa34949f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1137_jpg.rf.87a9628e752f7e41a2f249ac96053d80.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1140_jpg.rf.6c8251c5ffeff3130faf5f0ce63a4b40.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1156_jpg.rf.c4e6ef1926523bc9a1b80e1206049ce2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay120_jpg.rf.8aaf3145a1a04e85cfb77ed104b723fe.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay122_jpg.rf.111e675e66e848134547669236e3517a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1275_jpg.rf.676effc229289e46d5fdf0da82874d4d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1306_jpg.rf.bdc1b8c23534a7bb1147c450a6564aa0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1309_jpg.rf.40763c0ddfc6b9f1eccbf478b0ec0ec4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay135_jpg.rf.414a6cff309e61ea5cf7146e8834b5db.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1431_jpg.rf.c9cdf2d5c306d97c7a18581711313e28.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1445_jpg.rf.bdab6b512aab0e7f88fd97ace8331a14.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1448_jpg.rf.c3a05bfc45498f5f086d40788a76516c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay146_jpg.rf.02899473604396905c8e12e46b74e226.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay147_jpg.rf.3a8e2fcf69130933df5d5c18074859b7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay14_jpg.rf.bf845f4602726e4cdc7edf9763fd9269.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay151_jpg.rf.a2aebb7c02bcf0f66bb0a86e4e3a463f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1533_jpg.rf.925efab48ae98bfd98c4eaa341efe4bb.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay153_jpg.rf.5ce471e9dad4a95eda0a2395c31494e4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay154_jpg.rf.5318d303e023a75eb1eacb3e52c5b025.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay155_jpg.rf.fd8ddcbf43e60cab527eb68288101fd9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay162_jpg.rf.6a44485c34f591f56e63f4a8932e505a.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1678_jpg.rf.62259b23f0514ff6fdd48c98c094e23c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay170_jpg.rf.116ef09a582c7f58927ff9b78544015c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1773_jpg.rf.420efea9ec89401233c3650afc17200d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay178_jpg.rf.b55f23ebd1813c7258d548cb8f1b3189.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay17_jpg.rf.98be91283992345d661e7e26d25ccdfd.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay185_jpg.rf.b43fc838e9d55bbe6bfced2f3acb0db1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1860_jpg.rf.7a7eda357cc0df39e91689a26321078c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay188_jpg.rf.b2d84c649db8c83e8a03585373faceee.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay1983_jpg.rf.e2898705a4ead935567fdb5adf1f0425.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2010_jpg.rf.aef1b8cb610d2339d116efe1f7981467.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2016_jpg.rf.64110c79efc20cc62163aab5360c52f3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2044_jpg.rf.af87ba641be236ed699791752d198666.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay205_jpg.rf.6b708ceebeeaacf5e35904182cc1bbdb.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay207_jpg.rf.0c4688fd8bd05f8cd9cb357e5870fdc7.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2099_jpg.rf.344b9a7d533437df0ddf226706cc1370.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay20_jpg.rf.141fec5f33cda046578e81309a737ee6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2108_jpg.rf.22155aea23c996c139f4765861312e99.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay210_jpg.rf.fbe21939c7df2741a87924585db3c484.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2135_jpg.rf.1e0a4f7a166b3ec05a18ecce950cf72c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay213_jpg.rf.7200a4fff1f78bd0318fccf4663ffa29.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2177_jpg.rf.ac7e109a9564b92733af629846e22cf8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay217_jpg.rf.828fbce8b775d3f87d5f65571adc4ff8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay21_jpg.rf.86b886483a1f98e67632b786226e638b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2226_jpg.rf.a3d7595891ff41d363da9f476d4c6006.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay222_jpg.rf.9384133e10587f1db7ef35f9fac24232.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2239_jpg.rf.6d3fb21b36d3845c03f89ba572a22f9d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2241_jpg.rf.f39ecaa15929cd40d5399eb299be09d8.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay228_jpg.rf.76204e5d954efc8cc2ea24a231a5e8a2.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay229_jpg.rf.27f5491857ef997ed2a545fed3f3dade.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2363_jpg.rf.690a639b69739e465771f5a49caa2954.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2366_jpg.rf.ce50c5fa45da19571513ac9f86f38087.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2373_jpg.rf.5f681114f307c1dc7560c98f914474c4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay2386_jpg.rf.51cd177cad0a9c3b795d07d40dd63094.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay238_jpg.rf.e8fafdcddc7ad0d9c23c6e837f9732d9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay300_jpg.rf.e9a88316158620103546f47ecb9c5583.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay316_jpg.rf.a5dca585fbf9f63648df4080f1b896c0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay33_jpg.rf.d2565c01480230660184e9c91948e24d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay371_jpg.rf.878c6355439c68cd2703a7dc74ab28f9.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay45_jpg.rf.9be9881cfebe28aa65beb851e006081c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay503_jpg.rf.518b354db377a4445838529e469ad023.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay50_jpg.rf.b51e888ea15b7bbc68c54fe4d71d81e0.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay51_jpg.rf.3c7fece5de98936d4cdfe753b977b2e6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay56_jpg.rf.7c9f79cc1dafcccc5b82d72c361eb5a6.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay58_jpg.rf.4751fa15d9b51be4690e25b65762ef8f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay646_jpg.rf.4f1e9474b46a9fcda21d3a27065350ac.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay798_jpg.rf.442b3ded0a4d9b7d2d707a776da7c1e3.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay807_jpg.rf.7b4dfcb35a4ef5d86fafbe43b8daad90.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay80_jpg.rf.0ffd4071773e0f122fff085be9578c94.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay81_jpg.rf.21e88848f4b9bde43e3b14d110907c93.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay83_jpg.rf.bc4915fea0c8fbc031603721195628f1.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay842_jpg.rf.2e32e1c6e1ce12b7febc654b8a5e2c00.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay84_jpg.rf.e8a123283a29d0e1f43f0a6375c90e7c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay850_jpg.rf.e284c5e55d9dd6bc211d773c195d1509.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay85_jpg.rf.08e991694fc67b9cfcde49ec52995784.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay877_jpg.rf.8add25aeb341311df413e46bac4fad8c.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay90_jpg.rf.d3256b8ae7775732165c527727834109.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay91_jpg.rf.a9c636adf13d5d81fa167e506692a84f.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemay941_jpg.rf.6ddb9c1a24fd87848b1248abd8b5d946.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemayBigPlate12_jpg.rf.9dcaff3f882941bb64f924699a988e96.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemayBigPlate192_jpg.rf.e3704e304f245bd92a772bd2f90f7b9b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemayBigPlate208_jpg.rf.7cc9606c2c106e96645838a1180bc909.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemayBigPlate24_jpg.rf.8885b886f8a941bdacbd8a054d3c8234.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemayBigPlate40_jpg.rf.1a041089bb5529ada9344d67fa2d56a4.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemayBigPlate48_jpg.rf.605d2557ec7b379613f8ff0ec449e961.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/multi_violation_dataset/images/val/xemayBigPlate62_jpg.rf.7e37b7c463595763e4bde49da806120b.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 2. Possible class labels are 0-1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/multi_violation_dataset/labels/val.cache\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/50      2.53G       1.52      2.832      1.355         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.428     0.0156      0.117       0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/50      2.91G      1.477      2.108       1.31         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.637      0.396      0.499      0.263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/50      2.91G      1.511      2.056      1.351         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.601      0.525       0.55      0.316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/50      2.91G      1.538      1.885      1.348         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  2.98it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.581      0.565      0.532      0.284\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/50      2.91G      1.551      1.775      1.377         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.604      0.655      0.626      0.356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/50      2.91G      1.502      1.624      1.332          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.552      0.536      0.498      0.284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/50      2.91G       1.47      1.538      1.309         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  2.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.689      0.629      0.681      0.362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/50      2.91G      1.466      1.468      1.312         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.619      0.688      0.651      0.362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/50      2.91G       1.41      1.394       1.28         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.623      0.611      0.618      0.338\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/50      2.91G      1.449      1.404      1.307          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  2.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.622       0.72      0.659      0.366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/50      2.91G      1.426      1.326      1.291          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.734      0.689      0.703      0.391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/50      2.91G      1.425      1.302      1.278         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.677      0.717      0.714        0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/50      2.91G      1.393      1.283      1.268         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  3.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.614      0.694      0.698      0.378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/50      2.91G      1.369       1.25      1.247         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.675      0.728      0.706      0.397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/50      2.91G      1.383      1.215      1.262         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  3.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.674        0.7      0.683      0.381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/50      2.91G      1.352        1.2      1.241         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.648      0.671      0.651      0.368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/50      2.91G       1.32      1.132      1.235         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.709      0.709      0.753       0.42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/50      2.91G      1.344      1.148      1.244          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.721       0.77      0.772      0.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/50      2.91G      1.314      1.135      1.207         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:18<00:00,  2.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.701      0.758      0.762      0.419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/50      2.91G      1.327      1.102      1.241         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.696      0.725       0.71      0.398\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/50      2.93G      1.294      1.081      1.211         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  3.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.725      0.764      0.753       0.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/50      2.94G      1.279      1.034      1.203         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.691      0.684      0.701      0.394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/50      2.96G      1.307      1.065      1.218         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.698      0.699      0.709      0.369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/50      2.98G      1.276      1.032      1.196         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.702      0.726      0.761      0.424\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/50      2.99G      1.231      1.017       1.17         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  3.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.721      0.721       0.76      0.432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/50         3G      1.254     0.9921      1.196         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.774      0.724      0.763      0.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/50      3.03G      1.227     0.9846      1.184         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  2.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.765        0.7       0.75      0.428\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/50      3.05G      1.237     0.9689      1.175         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.732      0.788      0.783      0.436\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/50      3.06G      1.209     0.9439      1.158         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.757      0.731      0.782      0.438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/50      3.07G      1.192     0.9231      1.173         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  3.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.731      0.712      0.759      0.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      31/50       3.1G      1.207     0.8828      1.159         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.732      0.765      0.776      0.436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      32/50      3.12G      1.174     0.9099      1.154         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.756      0.746      0.767      0.423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      33/50      3.13G      1.158     0.8713       1.15         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  2.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.755      0.738      0.777      0.436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      34/50      3.14G       1.16     0.8552      1.141         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.693      0.794      0.774      0.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      35/50      3.17G      1.165     0.8542      1.151         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:18<00:00,  2.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.704      0.767      0.773      0.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      36/50      3.18G      1.161     0.8499       1.15         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.729      0.764      0.773      0.427\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      37/50       3.2G      1.138      0.823      1.132         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.736      0.737      0.771      0.439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      38/50      3.21G      1.123     0.8043      1.125         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.46it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346       0.76      0.717       0.77       0.44\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      39/50      3.23G      1.109     0.7971      1.127         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:17<00:00,  2.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.757      0.751      0.767      0.426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      40/50      3.25G      1.113     0.7729      1.109         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:18<00:00,  2.79it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.751      0.736      0.776      0.435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      41/50      3.27G      1.048      0.736      1.103          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:18<00:00,  2.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.668      0.771      0.748       0.42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      42/50      3.28G       1.04     0.6892       1.09         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.683      0.779      0.757      0.426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      43/50       3.3G      1.005     0.6579      1.079          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:15<00:00,  3.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346        0.7      0.761      0.761      0.423\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      44/50      3.32G      1.007      0.644      1.083          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:15<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.762      0.735      0.786      0.442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      45/50      3.34G     0.9974     0.6397      1.069          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:15<00:00,  3.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.717      0.768      0.768      0.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      46/50      3.35G     0.9776     0.6313      1.065          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.737      0.764      0.776      0.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      47/50      3.37G     0.9673     0.6048      1.069         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:15<00:00,  3.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.726      0.762      0.779      0.433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      48/50      3.39G     0.9629     0.5965      1.066          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:15<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.763      0.744      0.787      0.443\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      49/50      3.41G     0.9574     0.5802      1.055          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.756      0.721      0.783      0.436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      50/50      3.41G     0.9335     0.5658      1.044          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:16<00:00,  3.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.752      0.752      0.782       0.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "50 epochs completed in 0.267 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics 8.3.121 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        346      0.766      0.746       0.79      0.443\n",
            "             no-helmet         97        190      0.829      0.889      0.903      0.569\n",
            "         triple-riding         62        156      0.702      0.603      0.677      0.318\n",
            "Speed: 0.7ms preprocess, 2.7ms inference, 0.0ms loss, 4.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0, 1])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7d75b494b050>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,    0.033023,    0.016511,           0],\n",
              "       [          1,           1,           1, ...,   0.0028558,   0.0014279,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.41189,     0.41189,     0.50132, ...,           0,           0,           0],\n",
              "       [    0.23818,     0.23818,     0.30906, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.26045,     0.26045,     0.33693, ...,           1,           1,           1],\n",
              "       [    0.13716,     0.13716,     0.18782, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.98421,     0.98421,     0.97895, ...,           0,           0,           0],\n",
              "       [    0.90385,     0.90385,     0.87179, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.4780824053870496)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.56889,     0.31794])\n",
              "names: {0: 'no-helmet', 1: 'triple-riding'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.7655631269355316), 'metrics/recall(B)': np.float64(0.7464260513465557), 'metrics/mAP50(B)': np.float64(0.7900795279106884), 'metrics/mAP50-95(B)': np.float64(0.44341605843997867), 'fitness': np.float64(0.4780824053870496)}\n",
              "save_dir: PosixPath('runs/detect/train2')\n",
              "speed: {'preprocess': 0.6523396266675263, 'inference': 2.718205006670663, 'loss': 0.00036421333182564314, 'postprocess': 3.958355073336861}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "grWdD-WoAlm-",
        "outputId": "bdf91009-2f17-4d77-cf26-6d2fa14c5bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-57c3ef7d-27a9-479f-b03e-037e280ced1e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-57c3ef7d-27a9-479f-b03e-037e280ced1e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Tripple ride1.mp4 to Tripple ride1.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "vtXTFQVbAmzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸš© 1. Install and import dependencies\n",
        "!pip install ultralytics -q\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# ğŸš© 2. Load your trained model\n",
        "model = YOLO('/content/runs/detect/train2/weights/best.pt')  # <- Update path if needed\n",
        "\n",
        "# ğŸš© 3. Upload video for testing\n",
        "\n",
        "\n",
        "# ğŸš© 4. Setup video capture and output writer\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "out = cv2.VideoWriter(\"Trippledet_output.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "# ğŸš© 5. Define colors for each violation type\n",
        "class_colors = {\n",
        "    \"no-helmet\": (0, 0, 255),         # ğŸ”´ Red\n",
        "    \"triple-riding\": (0, 0, 255)    # ğŸŸ  Orange\n",
        "}\n",
        "\n",
        "# ğŸš© 6. Run detection and draw results\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    results = model(frame)[0]\n",
        "\n",
        "    for box in results.boxes:\n",
        "        cls_id = int(box.cls[0])\n",
        "        label = model.names[cls_id]\n",
        "        conf = float(box.conf[0])\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "\n",
        "        color = class_colors.get(label, (255, 255, 255))  # White if unknown class\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)\n",
        "        # Draw label\n",
        "        cv2.putText(frame, f\"{label} ({conf:.2f})\", (x1, y1 - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "files.download(\"Trippledet_output.mp4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86831
        },
        "collapsed": true,
        "id": "p-vVrK3EAsMB",
        "outputId": "b0450087-56b8-4104-ad34-7969c192fd89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.7ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.3ms\n",
            "Speed: 2.4ms preprocess, 8.3ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.6ms\n",
            "Speed: 2.9ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.2ms\n",
            "Speed: 4.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 3.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.3ms\n",
            "Speed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.6ms\n",
            "Speed: 3.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 3.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.8ms\n",
            "Speed: 3.0ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.7ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.1ms\n",
            "Speed: 2.3ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.1ms\n",
            "Speed: 2.6ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.5ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.7ms\n",
            "Speed: 2.5ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.3ms\n",
            "Speed: 2.7ms preprocess, 12.3ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.7ms\n",
            "Speed: 2.6ms preprocess, 12.7ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.3ms\n",
            "Speed: 3.8ms preprocess, 13.3ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.7ms\n",
            "Speed: 2.9ms preprocess, 13.7ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 12.2ms\n",
            "Speed: 2.9ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 12.4ms\n",
            "Speed: 3.3ms preprocess, 12.4ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.6ms\n",
            "Speed: 2.6ms preprocess, 12.6ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 14.7ms\n",
            "Speed: 2.6ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.7ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 4.1ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.3ms\n",
            "Speed: 2.7ms preprocess, 9.3ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.3ms\n",
            "Speed: 4.4ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.6ms preprocess, 9.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.2ms preprocess, 9.7ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 4.2ms preprocess, 8.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.8ms\n",
            "Speed: 2.6ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 15.1ms\n",
            "Speed: 2.7ms preprocess, 15.1ms inference, 4.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 15.3ms\n",
            "Speed: 2.6ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 4.1ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.8ms\n",
            "Speed: 2.5ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.4ms\n",
            "Speed: 2.6ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.8ms\n",
            "Speed: 2.7ms preprocess, 12.8ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.9ms\n",
            "Speed: 2.6ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.8ms\n",
            "Speed: 4.5ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.6ms\n",
            "Speed: 2.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 15.5ms\n",
            "Speed: 2.6ms preprocess, 15.5ms inference, 6.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.2ms\n",
            "Speed: 2.6ms preprocess, 13.2ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.7ms\n",
            "Speed: 2.5ms preprocess, 13.7ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.8ms\n",
            "Speed: 4.9ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.2ms\n",
            "Speed: 8.9ms preprocess, 11.2ms inference, 7.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 4.1ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.5ms\n",
            "Speed: 2.5ms preprocess, 12.5ms inference, 2.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.8ms\n",
            "Speed: 4.3ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 10.8ms\n",
            "Speed: 4.2ms preprocess, 10.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.3ms\n",
            "Speed: 4.2ms preprocess, 9.3ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.0ms\n",
            "Speed: 4.2ms preprocess, 11.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 4.2ms preprocess, 9.8ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.6ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.1ms\n",
            "Speed: 2.6ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 4.3ms preprocess, 9.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.0ms\n",
            "Speed: 4.4ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.1ms\n",
            "Speed: 5.4ms preprocess, 12.1ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.9ms\n",
            "Speed: 4.1ms preprocess, 10.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.1ms\n",
            "Speed: 4.3ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 11.7ms\n",
            "Speed: 6.7ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.1ms\n",
            "Speed: 4.0ms preprocess, 12.1ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.9ms\n",
            "Speed: 3.5ms preprocess, 10.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.6ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 2.6ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 3.2ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.6ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 3.1ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 4.3ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 3.7ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 3.2ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.2ms\n",
            "Speed: 2.6ms preprocess, 10.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 3.4ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.7ms preprocess, 8.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 2.9ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.9ms\n",
            "Speed: 3.9ms preprocess, 12.9ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.5ms\n",
            "Speed: 2.2ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.2ms\n",
            "Speed: 4.5ms preprocess, 13.2ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.5ms\n",
            "Speed: 2.5ms preprocess, 12.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 17.0ms\n",
            "Speed: 2.7ms preprocess, 17.0ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.5ms\n",
            "Speed: 2.4ms preprocess, 10.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.7ms\n",
            "Speed: 2.5ms preprocess, 14.7ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 15.6ms\n",
            "Speed: 2.6ms preprocess, 15.6ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.9ms\n",
            "Speed: 2.6ms preprocess, 12.9ms inference, 4.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.8ms\n",
            "Speed: 2.7ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 15.3ms\n",
            "Speed: 2.9ms preprocess, 15.3ms inference, 3.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.0ms\n",
            "Speed: 2.6ms preprocess, 12.0ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.6ms\n",
            "Speed: 2.5ms preprocess, 12.6ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.3ms\n",
            "Speed: 2.4ms preprocess, 12.3ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.6ms\n",
            "Speed: 2.4ms preprocess, 12.6ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.9ms\n",
            "Speed: 2.6ms preprocess, 13.9ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 15.4ms\n",
            "Speed: 4.6ms preprocess, 15.4ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.9ms\n",
            "Speed: 3.3ms preprocess, 13.9ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.0ms\n",
            "Speed: 1.8ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.8ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.7ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.8ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.8ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 6.9ms\n",
            "Speed: 2.1ms preprocess, 6.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.7ms\n",
            "Speed: 3.0ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 3.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.1ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.1ms\n",
            "Speed: 1.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.6ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.6ms\n",
            "Speed: 2.6ms preprocess, 13.6ms inference, 2.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.8ms\n",
            "Speed: 2.6ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.6ms\n",
            "Speed: 3.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.1ms\n",
            "Speed: 2.4ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.8ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.9ms\n",
            "Speed: 2.3ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.3ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.6ms\n",
            "Speed: 10.4ms preprocess, 11.6ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.7ms\n",
            "Speed: 2.4ms preprocess, 12.7ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.1ms\n",
            "Speed: 3.2ms preprocess, 14.1ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.3ms\n",
            "Speed: 2.6ms preprocess, 11.3ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.6ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 3.4ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.6ms\n",
            "Speed: 3.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.2ms\n",
            "Speed: 2.2ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.6ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.1ms\n",
            "Speed: 2.4ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.1ms\n",
            "Speed: 4.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 13.1ms\n",
            "Speed: 2.7ms preprocess, 13.1ms inference, 2.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.0ms\n",
            "Speed: 3.6ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.6ms\n",
            "Speed: 2.6ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.2ms\n",
            "Speed: 2.8ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 3.3ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.3ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 3.0ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.3ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.6ms\n",
            "Speed: 3.2ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.3ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.2ms\n",
            "Speed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.7ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.2ms\n",
            "Speed: 3.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.7ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.8ms\n",
            "Speed: 2.7ms preprocess, 14.8ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 3.3ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.2ms\n",
            "Speed: 3.2ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.9ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.2ms\n",
            "Speed: 3.1ms preprocess, 10.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 2.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.7ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 4.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.2ms\n",
            "Speed: 2.7ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.6ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 8.3ms\n",
            "Speed: 2.6ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.2ms\n",
            "Speed: 1.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.8ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.4ms\n",
            "Speed: 7.0ms preprocess, 12.4ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.8ms\n",
            "Speed: 2.7ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 4.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.8ms\n",
            "Speed: 2.8ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 3.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.1ms\n",
            "Speed: 1.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 3.0ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.7ms\n",
            "Speed: 3.0ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 3.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.2ms\n",
            "Speed: 3.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.5ms\n",
            "Speed: 1.7ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.1ms\n",
            "Speed: 2.9ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.7ms\n",
            "Speed: 3.0ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.8ms\n",
            "Speed: 3.7ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.1ms\n",
            "Speed: 2.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.9ms\n",
            "Speed: 2.6ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.7ms\n",
            "Speed: 2.7ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.4ms\n",
            "Speed: 4.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.3ms\n",
            "Speed: 1.9ms preprocess, 9.3ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.6ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.7ms\n",
            "Speed: 2.4ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.6ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.8ms\n",
            "Speed: 2.6ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.6ms\n",
            "Speed: 2.7ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 4.1ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.3ms\n",
            "Speed: 2.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.3ms\n",
            "Speed: 2.5ms preprocess, 13.3ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.7ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.7ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.9ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.7ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.6ms\n",
            "Speed: 3.7ms preprocess, 12.6ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.9ms\n",
            "Speed: 2.6ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.7ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.8ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.8ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 2.8ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.7ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 3.7ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.8ms\n",
            "Speed: 2.8ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.5ms\n",
            "Speed: 2.9ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.7ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.6ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.1ms\n",
            "Speed: 2.6ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 12.2ms\n",
            "Speed: 2.9ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.2ms\n",
            "Speed: 2.6ms preprocess, 14.2ms inference, 2.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.7ms\n",
            "Speed: 2.7ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.3ms\n",
            "Speed: 3.1ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.6ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.0ms\n",
            "Speed: 2.8ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.1ms\n",
            "Speed: 2.7ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.1ms\n",
            "Speed: 3.5ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.7ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 3.0ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.7ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.5ms\n",
            "Speed: 2.9ms preprocess, 10.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 4.7ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 3.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 3.2ms preprocess, 9.6ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.8ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.6ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 3.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.8ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 1.9ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.0ms\n",
            "Speed: 3.5ms preprocess, 14.0ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.6ms\n",
            "Speed: 2.7ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.4ms\n",
            "Speed: 3.6ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.3ms\n",
            "Speed: 1.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.3ms\n",
            "Speed: 1.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.6ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.7ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.3ms\n",
            "Speed: 2.6ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.5ms\n",
            "Speed: 2.3ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.3ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 3.4ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.4ms\n",
            "Speed: 4.2ms preprocess, 13.4ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.7ms\n",
            "Speed: 3.1ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.5ms\n",
            "Speed: 3.2ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 6.9ms\n",
            "Speed: 2.0ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.7ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.1ms\n",
            "Speed: 1.9ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.9ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.3ms\n",
            "Speed: 2.4ms preprocess, 13.3ms inference, 2.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.4ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.3ms\n",
            "Speed: 2.6ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.4ms\n",
            "Speed: 2.5ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.6ms\n",
            "Speed: 3.7ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.6ms\n",
            "Speed: 2.0ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.9ms\n",
            "Speed: 2.6ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.2ms\n",
            "Speed: 2.6ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.0ms\n",
            "Speed: 2.7ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.3ms\n",
            "Speed: 2.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.0ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.3ms\n",
            "Speed: 2.8ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 13.2ms\n",
            "Speed: 3.3ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 18.0ms\n",
            "Speed: 2.4ms preprocess, 18.0ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.2ms\n",
            "Speed: 2.5ms preprocess, 13.2ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.4ms\n",
            "Speed: 2.4ms preprocess, 12.4ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 20.6ms\n",
            "Speed: 2.5ms preprocess, 20.6ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.8ms\n",
            "Speed: 2.5ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 19.0ms\n",
            "Speed: 2.5ms preprocess, 19.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.9ms\n",
            "Speed: 4.3ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 3.0ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.8ms\n",
            "Speed: 2.4ms preprocess, 12.8ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.8ms\n",
            "Speed: 2.5ms preprocess, 14.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 16.0ms\n",
            "Speed: 2.4ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.9ms\n",
            "Speed: 2.5ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 3.1ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.9ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 3.1ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.5ms\n",
            "Speed: 2.4ms preprocess, 11.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.6ms\n",
            "Speed: 2.9ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.9ms\n",
            "Speed: 3.1ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 3.1ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.5ms\n",
            "Speed: 3.2ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 3.2ms preprocess, 9.8ms inference, 3.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.7ms\n",
            "Speed: 2.8ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 4.8ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.0ms\n",
            "Speed: 6.1ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 4.3ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.8ms\n",
            "Speed: 3.5ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 12.9ms\n",
            "Speed: 2.5ms preprocess, 12.9ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 11.4ms\n",
            "Speed: 3.6ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 11.8ms\n",
            "Speed: 2.5ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 17.6ms\n",
            "Speed: 2.6ms preprocess, 17.6ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.6ms\n",
            "Speed: 2.5ms preprocess, 12.6ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 14.7ms\n",
            "Speed: 7.0ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 16.5ms\n",
            "Speed: 5.6ms preprocess, 16.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 22.1ms\n",
            "Speed: 3.2ms preprocess, 22.1ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.3ms\n",
            "Speed: 5.1ms preprocess, 14.3ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.4ms\n",
            "Speed: 2.5ms preprocess, 12.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.7ms\n",
            "Speed: 2.3ms preprocess, 11.7ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.3ms\n",
            "Speed: 2.6ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.3ms\n",
            "Speed: 4.3ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.9ms\n",
            "Speed: 2.5ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.8ms\n",
            "Speed: 2.4ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.3ms\n",
            "Speed: 3.0ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.2ms\n",
            "Speed: 2.5ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 15.7ms\n",
            "Speed: 2.4ms preprocess, 15.7ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.1ms\n",
            "Speed: 3.0ms preprocess, 11.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 4.7ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 6.3ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.7ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.3ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.7ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.3ms\n",
            "Speed: 2.7ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 17.3ms\n",
            "Speed: 2.6ms preprocess, 17.3ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.6ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.8ms\n",
            "Speed: 2.4ms preprocess, 13.8ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.3ms\n",
            "Speed: 4.9ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.5ms\n",
            "Speed: 2.4ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.8ms\n",
            "Speed: 2.5ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.1ms\n",
            "Speed: 2.8ms preprocess, 12.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.8ms preprocess, 9.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.6ms\n",
            "Speed: 2.7ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.5ms\n",
            "Speed: 2.7ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 3.5ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.4ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.4ms\n",
            "Speed: 2.4ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.6ms\n",
            "Speed: 2.5ms preprocess, 11.6ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.1ms\n",
            "Speed: 2.6ms preprocess, 14.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.4ms\n",
            "Speed: 2.4ms preprocess, 13.4ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.9ms\n",
            "Speed: 4.4ms preprocess, 12.9ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.3ms\n",
            "Speed: 2.4ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.1ms\n",
            "Speed: 2.5ms preprocess, 12.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.2ms\n",
            "Speed: 3.5ms preprocess, 14.2ms inference, 4.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 17.3ms\n",
            "Speed: 2.5ms preprocess, 17.3ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 14.1ms\n",
            "Speed: 2.6ms preprocess, 14.1ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.6ms\n",
            "Speed: 2.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.8ms\n",
            "Speed: 2.4ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.5ms\n",
            "Speed: 2.5ms preprocess, 12.5ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.1ms\n",
            "Speed: 2.1ms preprocess, 13.1ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.6ms\n",
            "Speed: 3.2ms preprocess, 13.6ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.2ms\n",
            "Speed: 2.5ms preprocess, 14.2ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.8ms\n",
            "Speed: 2.5ms preprocess, 11.8ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.4ms\n",
            "Speed: 2.7ms preprocess, 12.4ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 16.2ms\n",
            "Speed: 2.5ms preprocess, 16.2ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.7ms\n",
            "Speed: 2.6ms preprocess, 13.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.4ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 4.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.7ms\n",
            "Speed: 2.5ms preprocess, 11.7ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.6ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.8ms\n",
            "Speed: 3.1ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.7ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.1ms\n",
            "Speed: 2.5ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 3.2ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.3ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 3.0ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.3ms\n",
            "Speed: 2.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.9ms\n",
            "Speed: 2.4ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 11.7ms\n",
            "Speed: 2.7ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.0ms\n",
            "Speed: 1.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 6.9ms\n",
            "Speed: 1.9ms preprocess, 6.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 11.0ms\n",
            "Speed: 2.3ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.8ms\n",
            "Speed: 2.3ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.5ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.4ms\n",
            "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.5ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.6ms\n",
            "Speed: 2.5ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.2ms\n",
            "Speed: 5.5ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.7ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.4ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.7ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.5ms\n",
            "Speed: 2.5ms preprocess, 10.5ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.1ms\n",
            "Speed: 2.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.4ms\n",
            "Speed: 2.4ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.0ms\n",
            "Speed: 3.4ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.1ms\n",
            "Speed: 1.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.4ms\n",
            "Speed: 2.3ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 11.6ms\n",
            "Speed: 2.2ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.5ms\n",
            "Speed: 2.3ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.2ms\n",
            "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.6ms\n",
            "Speed: 2.4ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.6ms\n",
            "Speed: 2.7ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.5ms\n",
            "Speed: 8.3ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.1ms\n",
            "Speed: 1.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 16.3ms\n",
            "Speed: 6.2ms preprocess, 16.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.6ms\n",
            "Speed: 2.4ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.4ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.6ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.5ms\n",
            "Speed: 2.6ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.2ms\n",
            "Speed: 2.6ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.6ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.3ms\n",
            "Speed: 2.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 3.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 2.8ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.6ms\n",
            "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 3.1ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 5.4ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.7ms\n",
            "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.9ms\n",
            "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.0ms\n",
            "Speed: 2.6ms preprocess, 13.0ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.7ms\n",
            "Speed: 2.6ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.8ms\n",
            "Speed: 2.7ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 3.4ms preprocess, 8.7ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.7ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.0ms\n",
            "Speed: 1.8ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.9ms\n",
            "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.7ms\n",
            "Speed: 2.8ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.1ms\n",
            "Speed: 2.5ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 3.3ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.6ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.5ms\n",
            "Speed: 2.7ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.4ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 14.7ms\n",
            "Speed: 2.3ms preprocess, 14.7ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.3ms\n",
            "Speed: 2.1ms preprocess, 7.3ms inference, 2.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 3.5ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.6ms\n",
            "Speed: 2.5ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.5ms\n",
            "Speed: 2.3ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.1ms\n",
            "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.3ms\n",
            "Speed: 2.6ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.1ms\n",
            "Speed: 3.6ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.4ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.3ms\n",
            "Speed: 2.4ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.7ms\n",
            "Speed: 3.7ms preprocess, 10.7ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.6ms\n",
            "Speed: 2.4ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 3.7ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 3.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 2.3ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 3.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.2ms\n",
            "Speed: 4.3ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.9ms\n",
            "Speed: 2.3ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.8ms\n",
            "Speed: 3.9ms preprocess, 11.8ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.1ms\n",
            "Speed: 2.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.3ms\n",
            "Speed: 2.4ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.2ms\n",
            "Speed: 2.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 11.0ms\n",
            "Speed: 2.4ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.0ms\n",
            "Speed: 2.5ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 18.3ms\n",
            "Speed: 2.4ms preprocess, 18.3ms inference, 2.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.7ms\n",
            "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.6ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.9ms\n",
            "Speed: 3.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.6ms\n",
            "Speed: 2.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.8ms\n",
            "Speed: 2.6ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 8.3ms\n",
            "Speed: 2.4ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 11.3ms\n",
            "Speed: 2.6ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 7.2ms\n",
            "Speed: 1.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 5 triple-ridings, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 6.8ms\n",
            "Speed: 2.0ms preprocess, 6.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.4ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.1ms\n",
            "Speed: 1.9ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.5ms\n",
            "Speed: 3.2ms preprocess, 11.5ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 14.0ms\n",
            "Speed: 2.9ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.4ms\n",
            "Speed: 3.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.5ms\n",
            "Speed: 2.7ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.3ms\n",
            "Speed: 2.8ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.3ms\n",
            "Speed: 2.4ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.3ms\n",
            "Speed: 2.4ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.5ms\n",
            "Speed: 2.5ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.2ms\n",
            "Speed: 2.0ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.6ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.6ms\n",
            "Speed: 2.4ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.2ms\n",
            "Speed: 2.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.3ms\n",
            "Speed: 2.6ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.9ms\n",
            "Speed: 2.4ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.2ms\n",
            "Speed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 4.0ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 15.1ms\n",
            "Speed: 2.5ms preprocess, 15.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.7ms\n",
            "Speed: 2.5ms preprocess, 8.7ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 5.5ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.8ms\n",
            "Speed: 2.6ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.6ms\n",
            "Speed: 2.4ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.2ms\n",
            "Speed: 2.6ms preprocess, 9.2ms inference, 4.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.2ms\n",
            "Speed: 3.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.7ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.5ms\n",
            "Speed: 2.4ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.2ms\n",
            "Speed: 2.6ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.1ms\n",
            "Speed: 2.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.1ms\n",
            "Speed: 2.5ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 2.6ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.5ms\n",
            "Speed: 2.5ms preprocess, 10.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.8ms\n",
            "Speed: 2.3ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 2.6ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.9ms\n",
            "Speed: 2.5ms preprocess, 13.9ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.5ms\n",
            "Speed: 2.7ms preprocess, 13.5ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.8ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.9ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.2ms\n",
            "Speed: 2.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.4ms\n",
            "Speed: 2.6ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.6ms\n",
            "Speed: 2.7ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.9ms\n",
            "Speed: 5.1ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.5ms\n",
            "Speed: 2.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.4ms\n",
            "Speed: 3.5ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.6ms\n",
            "Speed: 2.7ms preprocess, 13.6ms inference, 2.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.8ms\n",
            "Speed: 5.9ms preprocess, 11.8ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.1ms\n",
            "Speed: 2.6ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.6ms\n",
            "Speed: 2.4ms preprocess, 12.6ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.0ms\n",
            "Speed: 2.7ms preprocess, 12.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 21.1ms\n",
            "Speed: 3.7ms preprocess, 21.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 13.2ms\n",
            "Speed: 8.1ms preprocess, 13.2ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.8ms\n",
            "Speed: 2.3ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 10.6ms\n",
            "Speed: 2.3ms preprocess, 10.6ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 14.5ms\n",
            "Speed: 2.5ms preprocess, 14.5ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 14.1ms\n",
            "Speed: 2.6ms preprocess, 14.1ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.2ms\n",
            "Speed: 2.5ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.0ms\n",
            "Speed: 2.4ms preprocess, 12.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.2ms\n",
            "Speed: 2.5ms preprocess, 11.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.9ms\n",
            "Speed: 2.4ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 19.9ms\n",
            "Speed: 2.3ms preprocess, 19.9ms inference, 3.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.3ms\n",
            "Speed: 2.4ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.8ms\n",
            "Speed: 2.6ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 4.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.8ms\n",
            "Speed: 2.4ms preprocess, 12.8ms inference, 2.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 16.0ms\n",
            "Speed: 2.6ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.2ms\n",
            "Speed: 2.5ms preprocess, 12.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.3ms\n",
            "Speed: 2.6ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.3ms\n",
            "Speed: 2.6ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.4ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 5.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 2.3ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.7ms\n",
            "Speed: 2.3ms preprocess, 11.7ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.3ms\n",
            "Speed: 4.7ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.2ms\n",
            "Speed: 2.4ms preprocess, 14.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.7ms\n",
            "Speed: 2.3ms preprocess, 11.7ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.9ms\n",
            "Speed: 3.6ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.9ms\n",
            "Speed: 4.4ms preprocess, 12.9ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.8ms\n",
            "Speed: 2.7ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.6ms\n",
            "Speed: 4.6ms preprocess, 10.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.9ms\n",
            "Speed: 2.5ms preprocess, 10.9ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.6ms\n",
            "Speed: 3.1ms preprocess, 15.6ms inference, 4.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 17.9ms\n",
            "Speed: 2.6ms preprocess, 17.9ms inference, 2.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.3ms\n",
            "Speed: 2.6ms preprocess, 14.3ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.0ms\n",
            "Speed: 3.6ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 1 triple-riding, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 1 triple-riding, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.2ms\n",
            "Speed: 2.5ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 6.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.4ms\n",
            "Speed: 2.6ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.0ms\n",
            "Speed: 2.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.8ms\n",
            "Speed: 2.4ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 16.5ms\n",
            "Speed: 2.6ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 17.0ms\n",
            "Speed: 2.8ms preprocess, 17.0ms inference, 2.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.8ms\n",
            "Speed: 2.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.5ms\n",
            "Speed: 2.4ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.5ms\n",
            "Speed: 2.6ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.2ms\n",
            "Speed: 2.6ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 4 triple-ridings, 11.1ms\n",
            "Speed: 2.8ms preprocess, 11.1ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.6ms\n",
            "Speed: 2.7ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.3ms\n",
            "Speed: 2.7ms preprocess, 9.3ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 2 triple-ridings, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 1 triple-riding, 10.0ms\n",
            "Speed: 2.6ms preprocess, 10.0ms inference, 4.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.6ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.6ms\n",
            "Speed: 2.6ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.3ms\n",
            "Speed: 5.9ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.7ms\n",
            "Speed: 2.9ms preprocess, 8.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 15.3ms\n",
            "Speed: 2.9ms preprocess, 15.3ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.0ms\n",
            "Speed: 2.7ms preprocess, 9.0ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.8ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 3.1ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 2.7ms preprocess, 9.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 16.2ms\n",
            "Speed: 3.3ms preprocess, 16.2ms inference, 2.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.7ms\n",
            "Speed: 2.7ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.7ms\n",
            "Speed: 2.7ms preprocess, 10.7ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.5ms\n",
            "Speed: 2.8ms preprocess, 12.5ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.9ms\n",
            "Speed: 2.5ms preprocess, 12.9ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.8ms\n",
            "Speed: 2.6ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.9ms\n",
            "Speed: 3.0ms preprocess, 15.9ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.4ms\n",
            "Speed: 4.0ms preprocess, 11.4ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 18.2ms\n",
            "Speed: 3.9ms preprocess, 18.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.5ms\n",
            "Speed: 3.0ms preprocess, 9.5ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.7ms\n",
            "Speed: 3.1ms preprocess, 14.7ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.9ms\n",
            "Speed: 2.5ms preprocess, 11.9ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.2ms\n",
            "Speed: 2.5ms preprocess, 11.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.2ms\n",
            "Speed: 2.6ms preprocess, 15.2ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 16.2ms\n",
            "Speed: 2.5ms preprocess, 16.2ms inference, 5.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.8ms\n",
            "Speed: 2.6ms preprocess, 12.8ms inference, 2.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.4ms\n",
            "Speed: 2.5ms preprocess, 15.4ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.1ms\n",
            "Speed: 2.5ms preprocess, 12.1ms inference, 2.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 1 triple-riding, 12.3ms\n",
            "Speed: 2.4ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 1 triple-riding, 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.3ms\n",
            "Speed: 2.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.5ms\n",
            "Speed: 3.5ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.9ms\n",
            "Speed: 2.4ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.1ms\n",
            "Speed: 2.5ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.9ms\n",
            "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.2ms\n",
            "Speed: 3.2ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.2ms\n",
            "Speed: 2.5ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.0ms\n",
            "Speed: 2.4ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.3ms\n",
            "Speed: 3.7ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.2ms\n",
            "Speed: 4.6ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.0ms\n",
            "Speed: 3.0ms preprocess, 13.0ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.6ms\n",
            "Speed: 2.6ms preprocess, 14.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.5ms\n",
            "Speed: 2.5ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.5ms\n",
            "Speed: 2.8ms preprocess, 10.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.2ms\n",
            "Speed: 2.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.2ms\n",
            "Speed: 2.6ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.4ms\n",
            "Speed: 2.4ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.9ms\n",
            "Speed: 2.5ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.2ms\n",
            "Speed: 2.4ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 3.3ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.3ms\n",
            "Speed: 2.4ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.4ms\n",
            "Speed: 2.5ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.3ms preprocess, 9.9ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.5ms\n",
            "Speed: 2.4ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.1ms\n",
            "Speed: 3.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 7.8ms\n",
            "Speed: 5.6ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.3ms\n",
            "Speed: 2.9ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 17.7ms\n",
            "Speed: 2.6ms preprocess, 17.7ms inference, 5.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.3ms\n",
            "Speed: 4.6ms preprocess, 13.3ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 19.6ms\n",
            "Speed: 2.7ms preprocess, 19.6ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.5ms\n",
            "Speed: 3.0ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.3ms\n",
            "Speed: 2.4ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.3ms\n",
            "Speed: 2.6ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.6ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.9ms\n",
            "Speed: 3.0ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.9ms\n",
            "Speed: 2.5ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.0ms\n",
            "Speed: 6.5ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.7ms\n",
            "Speed: 2.6ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.7ms\n",
            "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.1ms\n",
            "Speed: 3.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 3.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.3ms\n",
            "Speed: 2.4ms preprocess, 12.3ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.8ms\n",
            "Speed: 2.6ms preprocess, 13.8ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.9ms\n",
            "Speed: 2.4ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.2ms\n",
            "Speed: 2.4ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.4ms\n",
            "Speed: 2.8ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.0ms\n",
            "Speed: 2.4ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.8ms\n",
            "Speed: 5.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 11.2ms\n",
            "Speed: 2.5ms preprocess, 11.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.5ms\n",
            "Speed: 3.6ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.5ms\n",
            "Speed: 2.4ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.3ms\n",
            "Speed: 2.1ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.4ms\n",
            "Speed: 2.4ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.0ms\n",
            "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.3ms\n",
            "Speed: 2.6ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.9ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.1ms\n",
            "Speed: 2.6ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 2.6ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.6ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 3.1ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.8ms\n",
            "Speed: 2.6ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 17.2ms\n",
            "Speed: 2.7ms preprocess, 17.2ms inference, 2.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.5ms\n",
            "Speed: 3.8ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.5ms\n",
            "Speed: 2.4ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 2.7ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.8ms\n",
            "Speed: 2.5ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.9ms\n",
            "Speed: 2.3ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.4ms\n",
            "Speed: 2.4ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 10.2ms\n",
            "Speed: 2.5ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.5ms\n",
            "Speed: 2.5ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.0ms\n",
            "Speed: 2.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.2ms\n",
            "Speed: 2.8ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.9ms\n",
            "Speed: 2.8ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.5ms\n",
            "Speed: 3.8ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.7ms\n",
            "Speed: 5.8ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.2ms\n",
            "Speed: 5.3ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.3ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.8ms\n",
            "Speed: 2.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.1ms\n",
            "Speed: 2.9ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.8ms\n",
            "Speed: 2.4ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.3ms\n",
            "Speed: 3.2ms preprocess, 13.3ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.4ms\n",
            "Speed: 2.9ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.2ms\n",
            "Speed: 2.6ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.2ms\n",
            "Speed: 2.5ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 12.4ms\n",
            "Speed: 2.8ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.0ms\n",
            "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.7ms\n",
            "Speed: 2.6ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.6ms\n",
            "Speed: 2.4ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.4ms\n",
            "Speed: 2.6ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.6ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.1ms\n",
            "Speed: 2.4ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.5ms\n",
            "Speed: 2.4ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.3ms\n",
            "Speed: 2.2ms preprocess, 11.3ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 17.0ms\n",
            "Speed: 2.5ms preprocess, 17.0ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.2ms\n",
            "Speed: 2.8ms preprocess, 11.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.0ms\n",
            "Speed: 3.6ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.6ms\n",
            "Speed: 2.7ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.0ms\n",
            "Speed: 2.5ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.4ms\n",
            "Speed: 3.2ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 6.9ms\n",
            "Speed: 2.4ms preprocess, 6.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.8ms\n",
            "Speed: 3.4ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.5ms\n",
            "Speed: 2.3ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.0ms\n",
            "Speed: 2.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.4ms\n",
            "Speed: 2.4ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.9ms\n",
            "Speed: 5.5ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.9ms\n",
            "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.1ms\n",
            "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.0ms\n",
            "Speed: 3.3ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 3.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.7ms\n",
            "Speed: 2.5ms preprocess, 14.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 3.2ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.7ms\n",
            "Speed: 2.3ms preprocess, 14.7ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 17.9ms\n",
            "Speed: 2.5ms preprocess, 17.9ms inference, 2.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.2ms\n",
            "Speed: 2.6ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.6ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.8ms\n",
            "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.9ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.3ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.5ms\n",
            "Speed: 2.5ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.6ms\n",
            "Speed: 2.5ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.4ms\n",
            "Speed: 2.4ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.3ms\n",
            "Speed: 2.9ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.9ms\n",
            "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 2.9ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.4ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.5ms\n",
            "Speed: 2.4ms preprocess, 11.5ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 17.7ms\n",
            "Speed: 2.5ms preprocess, 17.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.6ms\n",
            "Speed: 2.5ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 2 triple-ridings, 9.2ms\n",
            "Speed: 2.7ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.5ms\n",
            "Speed: 2.4ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.4ms\n",
            "Speed: 2.4ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.0ms\n",
            "Speed: 3.0ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.8ms\n",
            "Speed: 2.4ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.0ms\n",
            "Speed: 2.3ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.6ms\n",
            "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.7ms\n",
            "Speed: 2.4ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.4ms\n",
            "Speed: 2.4ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.7ms\n",
            "Speed: 3.2ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.1ms\n",
            "Speed: 2.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.0ms\n",
            "Speed: 2.4ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.0ms\n",
            "Speed: 2.4ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.0ms\n",
            "Speed: 2.3ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.4ms\n",
            "Speed: 2.4ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.8ms\n",
            "Speed: 2.7ms preprocess, 15.8ms inference, 2.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.7ms\n",
            "Speed: 3.2ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.0ms\n",
            "Speed: 2.6ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.5ms\n",
            "Speed: 2.4ms preprocess, 10.5ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.6ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.7ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.6ms\n",
            "Speed: 2.6ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.8ms\n",
            "Speed: 2.6ms preprocess, 9.8ms inference, 3.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.8ms\n",
            "Speed: 2.8ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.8ms\n",
            "Speed: 2.5ms preprocess, 11.8ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.5ms\n",
            "Speed: 2.6ms preprocess, 12.5ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 16.0ms\n",
            "Speed: 3.4ms preprocess, 16.0ms inference, 3.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 17.1ms\n",
            "Speed: 3.6ms preprocess, 17.1ms inference, 2.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.5ms\n",
            "Speed: 3.1ms preprocess, 13.5ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.8ms\n",
            "Speed: 3.2ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 3.1ms preprocess, 10.3ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.0ms\n",
            "Speed: 2.6ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.6ms\n",
            "Speed: 2.3ms preprocess, 10.6ms inference, 2.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.3ms\n",
            "Speed: 2.4ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 16.8ms\n",
            "Speed: 2.5ms preprocess, 16.8ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.1ms\n",
            "Speed: 2.4ms preprocess, 15.1ms inference, 3.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.5ms\n",
            "Speed: 4.3ms preprocess, 9.5ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.2ms\n",
            "Speed: 7.3ms preprocess, 10.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.2ms\n",
            "Speed: 2.5ms preprocess, 10.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.1ms\n",
            "Speed: 3.0ms preprocess, 11.1ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 3.0ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.7ms\n",
            "Speed: 2.6ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.4ms\n",
            "Speed: 2.6ms preprocess, 11.4ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.3ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.6ms\n",
            "Speed: 2.4ms preprocess, 10.6ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.4ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.6ms\n",
            "Speed: 2.4ms preprocess, 14.6ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.3ms\n",
            "Speed: 2.4ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.5ms\n",
            "Speed: 2.3ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.2ms\n",
            "Speed: 2.5ms preprocess, 11.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.7ms\n",
            "Speed: 2.4ms preprocess, 12.7ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.6ms\n",
            "Speed: 2.7ms preprocess, 12.6ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.8ms\n",
            "Speed: 2.5ms preprocess, 14.8ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.2ms\n",
            "Speed: 2.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.3ms\n",
            "Speed: 2.4ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.0ms\n",
            "Speed: 2.4ms preprocess, 11.0ms inference, 2.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.5ms\n",
            "Speed: 2.4ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.3ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.6ms\n",
            "Speed: 2.4ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.8ms\n",
            "Speed: 4.2ms preprocess, 11.8ms inference, 3.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.4ms\n",
            "Speed: 2.4ms preprocess, 12.4ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 16.2ms\n",
            "Speed: 2.4ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.9ms\n",
            "Speed: 2.5ms preprocess, 14.9ms inference, 2.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.5ms\n",
            "Speed: 3.1ms preprocess, 10.5ms inference, 5.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.9ms\n",
            "Speed: 2.5ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 16.2ms\n",
            "Speed: 2.5ms preprocess, 16.2ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 16.5ms\n",
            "Speed: 2.4ms preprocess, 16.5ms inference, 4.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.2ms\n",
            "Speed: 2.8ms preprocess, 14.2ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.1ms\n",
            "Speed: 2.6ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 20.8ms\n",
            "Speed: 2.5ms preprocess, 20.8ms inference, 4.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.8ms\n",
            "Speed: 2.4ms preprocess, 14.8ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.6ms\n",
            "Speed: 2.5ms preprocess, 13.6ms inference, 3.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.9ms\n",
            "Speed: 2.5ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 16.8ms\n",
            "Speed: 2.5ms preprocess, 16.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.2ms\n",
            "Speed: 3.6ms preprocess, 9.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.1ms\n",
            "Speed: 3.4ms preprocess, 11.1ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.0ms\n",
            "Speed: 2.9ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.4ms preprocess, 10.4ms inference, 5.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.0ms\n",
            "Speed: 2.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.0ms\n",
            "Speed: 2.4ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.3ms\n",
            "Speed: 2.6ms preprocess, 13.3ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 16.4ms\n",
            "Speed: 2.5ms preprocess, 16.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.4ms\n",
            "Speed: 2.4ms preprocess, 11.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.6ms\n",
            "Speed: 2.7ms preprocess, 13.6ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.8ms\n",
            "Speed: 2.4ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.1ms\n",
            "Speed: 2.3ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.5ms\n",
            "Speed: 2.3ms preprocess, 10.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.4ms\n",
            "Speed: 2.4ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.9ms\n",
            "Speed: 2.6ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.3ms\n",
            "Speed: 5.0ms preprocess, 10.3ms inference, 2.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.6ms\n",
            "Speed: 2.5ms preprocess, 13.6ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.8ms\n",
            "Speed: 2.2ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.3ms\n",
            "Speed: 2.4ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.4ms\n",
            "Speed: 2.3ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.8ms\n",
            "Speed: 3.4ms preprocess, 9.8ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.7ms\n",
            "Speed: 2.4ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 17.4ms\n",
            "Speed: 3.8ms preprocess, 17.4ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 8.7ms\n",
            "Speed: 3.6ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.8ms\n",
            "Speed: 2.6ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.0ms\n",
            "Speed: 2.4ms preprocess, 12.0ms inference, 4.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 17.7ms\n",
            "Speed: 2.9ms preprocess, 17.7ms inference, 6.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 3.4ms preprocess, 9.8ms inference, 5.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.2ms\n",
            "Speed: 3.6ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 3.6ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 13.5ms\n",
            "Speed: 2.4ms preprocess, 13.5ms inference, 2.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.7ms\n",
            "Speed: 2.6ms preprocess, 14.7ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.9ms\n",
            "Speed: 2.4ms preprocess, 13.9ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.3ms\n",
            "Speed: 2.2ms preprocess, 15.3ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 16.7ms\n",
            "Speed: 4.3ms preprocess, 16.7ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.2ms\n",
            "Speed: 2.8ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.0ms\n",
            "Speed: 2.1ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.2ms\n",
            "Speed: 2.3ms preprocess, 10.2ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 19.8ms\n",
            "Speed: 2.5ms preprocess, 19.8ms inference, 2.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.8ms\n",
            "Speed: 2.3ms preprocess, 8.8ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.3ms preprocess, 8.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.6ms\n",
            "Speed: 2.3ms preprocess, 12.6ms inference, 4.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.6ms\n",
            "Speed: 2.5ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 17.8ms\n",
            "Speed: 2.9ms preprocess, 17.8ms inference, 5.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 21.9ms\n",
            "Speed: 2.6ms preprocess, 21.9ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 18.2ms\n",
            "Speed: 2.4ms preprocess, 18.2ms inference, 4.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.0ms\n",
            "Speed: 2.3ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.2ms\n",
            "Speed: 4.5ms preprocess, 14.2ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.9ms\n",
            "Speed: 2.5ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.5ms\n",
            "Speed: 2.1ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.7ms\n",
            "Speed: 2.8ms preprocess, 12.7ms inference, 7.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 15.1ms\n",
            "Speed: 2.3ms preprocess, 15.1ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 13.4ms\n",
            "Speed: 2.5ms preprocess, 13.4ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.2ms\n",
            "Speed: 2.4ms preprocess, 12.2ms inference, 2.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.1ms\n",
            "Speed: 2.7ms preprocess, 12.1ms inference, 2.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 19.1ms\n",
            "Speed: 3.1ms preprocess, 19.1ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 14.4ms\n",
            "Speed: 2.5ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.3ms\n",
            "Speed: 4.8ms preprocess, 12.3ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 16.0ms\n",
            "Speed: 3.6ms preprocess, 16.0ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.4ms\n",
            "Speed: 2.6ms preprocess, 10.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.6ms\n",
            "Speed: 2.6ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 4.8ms preprocess, 10.3ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.9ms\n",
            "Speed: 4.6ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.1ms\n",
            "Speed: 2.4ms preprocess, 9.1ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.6ms\n",
            "Speed: 2.3ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.8ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.4ms\n",
            "Speed: 2.4ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.2ms\n",
            "Speed: 2.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 3 triple-ridings, 8.5ms\n",
            "Speed: 2.4ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 7.7ms\n",
            "Speed: 2.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 10.5ms\n",
            "Speed: 2.5ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.6ms\n",
            "Speed: 2.5ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.5ms\n",
            "Speed: 2.5ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.8ms\n",
            "Speed: 3.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.8ms\n",
            "Speed: 2.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 11.4ms\n",
            "Speed: 2.4ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 triple-ridings, 12.7ms\n",
            "Speed: 2.6ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 1 triple-riding, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 2.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 1 triple-riding, 13.8ms\n",
            "Speed: 2.4ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 1 triple-riding, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 1 triple-riding, 10.2ms\n",
            "Speed: 2.5ms preprocess, 10.2ms inference, 3.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.5ms\n",
            "Speed: 2.4ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.6ms\n",
            "Speed: 2.6ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.7ms\n",
            "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.0ms\n",
            "Speed: 2.6ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.9ms\n",
            "Speed: 2.6ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.7ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.5ms\n",
            "Speed: 2.4ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.9ms\n",
            "Speed: 2.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.8ms\n",
            "Speed: 2.4ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.9ms\n",
            "Speed: 2.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.8ms\n",
            "Speed: 2.1ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 12.0ms\n",
            "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.6ms\n",
            "Speed: 2.3ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.6ms\n",
            "Speed: 2.4ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.5ms\n",
            "Speed: 2.6ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.0ms\n",
            "Speed: 3.2ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 2.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.6ms\n",
            "Speed: 2.6ms preprocess, 11.6ms inference, 2.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 8.7ms\n",
            "Speed: 2.3ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.2ms\n",
            "Speed: 2.7ms preprocess, 9.2ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.3ms\n",
            "Speed: 2.6ms preprocess, 8.3ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.5ms\n",
            "Speed: 2.5ms preprocess, 10.5ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 12.0ms\n",
            "Speed: 2.4ms preprocess, 12.0ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.9ms\n",
            "Speed: 2.5ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 triple-riding, 10.3ms\n",
            "Speed: 2.6ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.9ms\n",
            "Speed: 3.4ms preprocess, 8.9ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.3ms\n",
            "Speed: 3.4ms preprocess, 10.3ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.9ms\n",
            "Speed: 3.4ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 2 no-helmets, 9.8ms\n",
            "Speed: 3.0ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 11.8ms\n",
            "Speed: 3.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 10.6ms\n",
            "Speed: 2.6ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 11.7ms\n",
            "Speed: 2.8ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 1 no-helmet, 9.7ms\n",
            "Speed: 3.3ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.9ms\n",
            "Speed: 2.7ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.6ms\n",
            "Speed: 3.0ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.9ms\n",
            "Speed: 2.5ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 13.4ms\n",
            "Speed: 2.3ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 11.5ms\n",
            "Speed: 3.2ms preprocess, 11.5ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.6ms\n",
            "Speed: 2.4ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.8ms\n",
            "Speed: 2.7ms preprocess, 8.8ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.5ms\n",
            "Speed: 2.5ms preprocess, 9.5ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 11.6ms\n",
            "Speed: 2.4ms preprocess, 11.6ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.0ms\n",
            "Speed: 2.1ms preprocess, 10.0ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.5ms\n",
            "Speed: 2.4ms preprocess, 8.5ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.4ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.2ms\n",
            "Speed: 2.4ms preprocess, 8.2ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.4ms\n",
            "Speed: 2.3ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.6ms\n",
            "Speed: 2.6ms preprocess, 9.6ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.3ms\n",
            "Speed: 2.7ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.9ms\n",
            "Speed: 2.6ms preprocess, 9.9ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.2ms\n",
            "Speed: 2.5ms preprocess, 10.2ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.3ms\n",
            "Speed: 2.3ms preprocess, 9.3ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 13.0ms\n",
            "Speed: 2.9ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.4ms\n",
            "Speed: 2.4ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.3ms\n",
            "Speed: 2.4ms preprocess, 8.3ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.8ms\n",
            "Speed: 2.3ms preprocess, 8.8ms inference, 2.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 16.8ms\n",
            "Speed: 2.5ms preprocess, 16.8ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.7ms\n",
            "Speed: 2.5ms preprocess, 8.7ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.7ms\n",
            "Speed: 2.9ms preprocess, 10.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 7.8ms\n",
            "Speed: 2.7ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 11.4ms\n",
            "Speed: 2.4ms preprocess, 11.4ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.0ms\n",
            "Speed: 2.5ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.5ms\n",
            "Speed: 2.7ms preprocess, 9.5ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.5ms\n",
            "Speed: 2.3ms preprocess, 8.5ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.2ms\n",
            "Speed: 2.3ms preprocess, 9.2ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.3ms\n",
            "Speed: 2.4ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.4ms\n",
            "Speed: 2.4ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.2ms\n",
            "Speed: 2.3ms preprocess, 9.2ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.5ms\n",
            "Speed: 2.1ms preprocess, 9.5ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.3ms\n",
            "Speed: 2.4ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.2ms\n",
            "Speed: 2.5ms preprocess, 9.2ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 13.2ms\n",
            "Speed: 2.7ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.6ms\n",
            "Speed: 2.4ms preprocess, 10.6ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.0ms\n",
            "Speed: 2.7ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.3ms\n",
            "Speed: 2.6ms preprocess, 10.3ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 15.3ms\n",
            "Speed: 2.4ms preprocess, 15.3ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 15.4ms\n",
            "Speed: 2.7ms preprocess, 15.4ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 11.7ms\n",
            "Speed: 2.4ms preprocess, 11.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.9ms\n",
            "Speed: 2.3ms preprocess, 9.9ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 17.7ms\n",
            "Speed: 2.5ms preprocess, 17.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.6ms preprocess, 9.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 3.2ms preprocess, 9.7ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.4ms\n",
            "Speed: 1.9ms preprocess, 10.4ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.6ms\n",
            "Speed: 2.4ms preprocess, 9.6ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 12.4ms\n",
            "Speed: 2.2ms preprocess, 12.4ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.3ms\n",
            "Speed: 2.4ms preprocess, 9.3ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.1ms\n",
            "Speed: 2.5ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.1ms\n",
            "Speed: 2.3ms preprocess, 9.1ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.2ms\n",
            "Speed: 2.7ms preprocess, 9.2ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.4ms\n",
            "Speed: 2.4ms preprocess, 10.4ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.5ms\n",
            "Speed: 2.4ms preprocess, 9.5ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.9ms\n",
            "Speed: 2.2ms preprocess, 9.9ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.2ms preprocess, 9.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.7ms\n",
            "Speed: 2.0ms preprocess, 10.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 12.9ms\n",
            "Speed: 2.7ms preprocess, 12.9ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.8ms\n",
            "Speed: 2.7ms preprocess, 9.8ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.3ms\n",
            "Speed: 2.0ms preprocess, 10.3ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 11.4ms\n",
            "Speed: 2.1ms preprocess, 11.4ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.3ms\n",
            "Speed: 2.0ms preprocess, 10.3ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.7ms\n",
            "Speed: 2.5ms preprocess, 8.7ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 13.3ms\n",
            "Speed: 2.1ms preprocess, 13.3ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 14.2ms\n",
            "Speed: 2.6ms preprocess, 14.2ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 14.0ms\n",
            "Speed: 2.5ms preprocess, 14.0ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.0ms\n",
            "Speed: 2.1ms preprocess, 9.0ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.1ms\n",
            "Speed: 2.2ms preprocess, 9.1ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.8ms\n",
            "Speed: 2.5ms preprocess, 8.8ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.5ms\n",
            "Speed: 2.2ms preprocess, 9.5ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.8ms\n",
            "Speed: 2.4ms preprocess, 9.8ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.0ms\n",
            "Speed: 2.2ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 11.4ms\n",
            "Speed: 2.4ms preprocess, 11.4ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.4ms\n",
            "Speed: 2.3ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.4ms\n",
            "Speed: 2.4ms preprocess, 10.4ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.9ms\n",
            "Speed: 2.3ms preprocess, 9.9ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.1ms preprocess, 9.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.9ms\n",
            "Speed: 2.3ms preprocess, 10.9ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.8ms\n",
            "Speed: 2.3ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.9ms\n",
            "Speed: 2.2ms preprocess, 8.9ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.4ms\n",
            "Speed: 2.2ms preprocess, 9.4ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.8ms\n",
            "Speed: 2.5ms preprocess, 9.8ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 11.5ms\n",
            "Speed: 2.5ms preprocess, 11.5ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.2ms\n",
            "Speed: 2.1ms preprocess, 10.2ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.4ms\n",
            "Speed: 4.0ms preprocess, 10.4ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.5ms\n",
            "Speed: 2.3ms preprocess, 9.5ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "0: 288x640 (no detections), 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d2332555-865c-4633-aefc-76100289e7ee\", \"Trippledet_output.mp4\", 68586075)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a-4ZZuArAuzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Red Light rossing**\n"
      ],
      "metadata": {
        "id": "xEEg_vYFA1OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== SETUP ==================\n",
        "!pip install -q ultralytics opencv-python-headless supervision\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from collections import defaultdict\n",
        "\n",
        "# ================== UPLOAD VIDEO ==================\n",
        "uploaded = files.upload()\n",
        "input_video_path = next(iter(uploaded))\n",
        "output_video_path = \"output_redlightcross.mp4\"\n",
        "\n",
        "# ================== LOAD YOLO MODEL ==================\n",
        "model = YOLO(\"yolov8n.pt\")  # Use yolov8m.pt for better accuracy if needed\n",
        "\n",
        "# ================== DETECTION FUNCTIONS ==================\n",
        "def is_red_light_on(frame, box):\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    light_roi = frame[y1:y2, x1:x2]\n",
        "    if light_roi.size == 0:\n",
        "        return False\n",
        "    hsv = cv2.cvtColor(light_roi, cv2.COLOR_BGR2HSV)\n",
        "    mask1 = cv2.inRange(hsv, (0, 70, 50), (10, 255, 255))\n",
        "    mask2 = cv2.inRange(hsv, (160, 70, 50), (180, 255, 255))\n",
        "    red_mask = cv2.bitwise_or(mask1, mask2)\n",
        "    red_ratio = np.sum(red_mask > 0) / (light_roi.shape[0] * light_roi.shape[1])\n",
        "    return red_ratio > 0.1\n",
        "\n",
        "def is_maroon_car(frame, box):\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    car_roi = frame[y1:y2, x1:x2]\n",
        "    if car_roi.size == 0:\n",
        "        return False\n",
        "    hsv = cv2.cvtColor(car_roi, cv2.COLOR_BGR2HSV)\n",
        "    mask1 = cv2.inRange(hsv, (0, 70, 50), (10, 255, 255))\n",
        "    mask2 = cv2.inRange(hsv, (160, 70, 50), (180, 255, 255))\n",
        "    red_mask = cv2.bitwise_or(mask1, mask2)\n",
        "    red_ratio = np.sum(red_mask > 0) / (car_roi.shape[0] * car_roi.shape[1])\n",
        "    return red_ratio > 0.2\n",
        "\n",
        "# ================== PROCESS VIDEO ==================\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width, height = int(cap.get(3)), int(cap.get(4))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "frame_id = 0\n",
        "track_history = defaultdict(list)\n",
        "red_light_on = False\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_id += 1\n",
        "\n",
        "    results = model(frame)[0]\n",
        "\n",
        "    # Check for red traffic light\n",
        "    red_light_on = False\n",
        "    for cls, box in zip(results.boxes.cls, results.boxes.xyxy):\n",
        "        if int(cls) == 9:  # Traffic light\n",
        "            if is_red_light_on(frame, box):\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                red_light_on = True\n",
        "                break\n",
        "\n",
        "    # Track vehicles\n",
        "    for i, (cls, box) in enumerate(zip(results.boxes.cls, results.boxes.xyxy)):\n",
        "        if int(cls) not in [2, 3, 5, 7]:  # car, motorcycle, bus, truck\n",
        "            continue\n",
        "\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "        track_id = i  # Simple index-based tracking\n",
        "        track_history[track_id].append((cx, cy))\n",
        "        if len(track_history[track_id]) > 10:\n",
        "            track_history[track_id] = track_history[track_id][-10:]\n",
        "\n",
        "        color = (0, 255, 0)  # Default green\n",
        "\n",
        "        # If it's a maroon car and red light is ON and it moves forward\n",
        "        if red_light_on and is_maroon_car(frame, box):\n",
        "            if len(track_history[track_id]) >= 2:\n",
        "                y_prev = track_history[track_id][-2][1]\n",
        "                y_curr = track_history[track_id][-1][1]\n",
        "                if y_curr < y_prev:  # Moving up (toward light)\n",
        "                    color = (0, 0, 255)\n",
        "                    cv2.putText(frame, \"Violator\", (x1, y1 - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(\"âœ… Done! Download your output video below.\")\n",
        "files.download(output_video_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g1a231A4WZ28",
        "outputId": "76396c28-773b-4a68-f2d9-3708593a0a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6153863d-69e2-49fa-b181-3cc8719b5486\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6153863d-69e2-49fa-b181-3cc8719b5486\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Red Light Violation.mp4 to Red Light Violation.mp4\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 20.2ms\n",
            "Speed: 2.5ms preprocess, 20.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 7.4ms\n",
            "Speed: 2.8ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 8.1ms\n",
            "Speed: 4.9ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 truck, 3 traffic lights, 12.6ms\n",
            "Speed: 3.0ms preprocess, 12.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 2 trucks, 3 traffic lights, 7.7ms\n",
            "Speed: 3.9ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 2 trucks, 3 traffic lights, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 2 trucks, 3 traffic lights, 8.2ms\n",
            "Speed: 3.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 2 trucks, 3 traffic lights, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 2 trucks, 3 traffic lights, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 2 trucks, 3 traffic lights, 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 2 trucks, 3 traffic lights, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 2 trucks, 3 traffic lights, 7.8ms\n",
            "Speed: 4.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 2 trucks, 3 traffic lights, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 2 trucks, 3 traffic lights, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 2 trucks, 3 traffic lights, 10.4ms\n",
            "Speed: 3.4ms preprocess, 10.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 2 trucks, 3 traffic lights, 8.5ms\n",
            "Speed: 3.0ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 2 trucks, 3 traffic lights, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 2 trucks, 3 traffic lights, 8.4ms\n",
            "Speed: 4.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 2 trucks, 3 traffic lights, 8.5ms\n",
            "Speed: 2.8ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 2 trucks, 3 traffic lights, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 2 trucks, 3 traffic lights, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 2 trucks, 3 traffic lights, 8.5ms\n",
            "Speed: 3.2ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 2 trucks, 4 traffic lights, 8.6ms\n",
            "Speed: 3.3ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 2 trucks, 3 traffic lights, 8.6ms\n",
            "Speed: 3.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 2 trucks, 4 traffic lights, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 2 trucks, 4 traffic lights, 9.4ms\n",
            "Speed: 3.1ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 2 trucks, 4 traffic lights, 8.5ms\n",
            "Speed: 3.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 2 trucks, 4 traffic lights, 12.1ms\n",
            "Speed: 3.2ms preprocess, 12.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 2 trucks, 4 traffic lights, 8.2ms\n",
            "Speed: 3.6ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 4 traffic lights, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 4 traffic lights, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 4 traffic lights, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 4 traffic lights, 9.2ms\n",
            "Speed: 3.5ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 4 traffic lights, 8.1ms\n",
            "Speed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 4 traffic lights, 9.7ms\n",
            "Speed: 3.5ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 4 traffic lights, 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 5 traffic lights, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 5 traffic lights, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 5 traffic lights, 10.2ms\n",
            "Speed: 3.1ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 5 traffic lights, 12.1ms\n",
            "Speed: 4.6ms preprocess, 12.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 4 traffic lights, 12.6ms\n",
            "Speed: 4.8ms preprocess, 12.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 truck, 4 traffic lights, 14.8ms\n",
            "Speed: 4.0ms preprocess, 14.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 4 traffic lights, 10.5ms\n",
            "Speed: 3.2ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 truck, 4 traffic lights, 14.3ms\n",
            "Speed: 3.1ms preprocess, 14.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 truck, 4 traffic lights, 14.4ms\n",
            "Speed: 3.7ms preprocess, 14.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 4 traffic lights, 10.2ms\n",
            "Speed: 3.1ms preprocess, 10.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 4 traffic lights, 9.3ms\n",
            "Speed: 3.5ms preprocess, 9.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 truck, 4 traffic lights, 13.3ms\n",
            "Speed: 3.0ms preprocess, 13.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 truck, 4 traffic lights, 13.3ms\n",
            "Speed: 3.0ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 4 traffic lights, 11.4ms\n",
            "Speed: 3.0ms preprocess, 11.4ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 truck, 4 traffic lights, 9.2ms\n",
            "Speed: 3.0ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 4 traffic lights, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 4 traffic lights, 11.3ms\n",
            "Speed: 3.2ms preprocess, 11.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 4 traffic lights, 10.1ms\n",
            "Speed: 3.1ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 9.5ms\n",
            "Speed: 3.2ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 bus, 1 truck, 4 traffic lights, 10.3ms\n",
            "Speed: 3.1ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 bus, 1 truck, 4 traffic lights, 11.9ms\n",
            "Speed: 3.2ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 bus, 1 truck, 4 traffic lights, 9.4ms\n",
            "Speed: 4.5ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 5 traffic lights, 9.6ms\n",
            "Speed: 4.4ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 5 traffic lights, 9.7ms\n",
            "Speed: 4.3ms preprocess, 9.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 5 traffic lights, 11.1ms\n",
            "Speed: 3.3ms preprocess, 11.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 4 traffic lights, 15.9ms\n",
            "Speed: 3.8ms preprocess, 15.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 4 traffic lights, 12.3ms\n",
            "Speed: 3.0ms preprocess, 12.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 4 traffic lights, 16.0ms\n",
            "Speed: 3.3ms preprocess, 16.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 5 traffic lights, 15.5ms\n",
            "Speed: 3.3ms preprocess, 15.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 13.7ms\n",
            "Speed: 3.1ms preprocess, 13.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 10.1ms\n",
            "Speed: 3.4ms preprocess, 10.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 4 traffic lights, 9.6ms\n",
            "Speed: 3.4ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 12.6ms\n",
            "Speed: 3.4ms preprocess, 12.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 11.0ms\n",
            "Speed: 3.6ms preprocess, 11.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 4 traffic lights, 12.8ms\n",
            "Speed: 3.6ms preprocess, 12.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 13.8ms\n",
            "Speed: 3.1ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 9.1ms\n",
            "Speed: 3.1ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 4 traffic lights, 9.1ms\n",
            "Speed: 3.1ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 9.1ms\n",
            "Speed: 4.0ms preprocess, 9.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 4 traffic lights, 9.1ms\n",
            "Speed: 3.9ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 9.5ms\n",
            "Speed: 3.7ms preprocess, 9.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 4 traffic lights, 12.6ms\n",
            "Speed: 3.7ms preprocess, 12.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 8.8ms\n",
            "Speed: 4.0ms preprocess, 8.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 9.2ms\n",
            "Speed: 3.6ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 8.9ms\n",
            "Speed: 3.7ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 8.5ms\n",
            "Speed: 3.7ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 8.5ms\n",
            "Speed: 3.0ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 3 traffic lights, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 3 traffic lights, 9.2ms\n",
            "Speed: 3.1ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 8.8ms\n",
            "Speed: 3.1ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 26.2ms\n",
            "Speed: 3.1ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 13.4ms\n",
            "Speed: 3.3ms preprocess, 13.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 bus, 1 truck, 3 traffic lights, 14.1ms\n",
            "Speed: 3.1ms preprocess, 14.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 10.3ms\n",
            "Speed: 3.0ms preprocess, 10.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 14.2ms\n",
            "Speed: 3.1ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 truck, 3 traffic lights, 8.8ms\n",
            "Speed: 3.1ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 12.9ms\n",
            "Speed: 4.5ms preprocess, 12.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 14.7ms\n",
            "Speed: 3.8ms preprocess, 14.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 10.2ms\n",
            "Speed: 3.0ms preprocess, 10.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 15.5ms\n",
            "Speed: 3.1ms preprocess, 15.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 13.5ms\n",
            "Speed: 3.6ms preprocess, 13.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 13.7ms\n",
            "Speed: 3.2ms preprocess, 13.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 12.9ms\n",
            "Speed: 3.1ms preprocess, 12.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 14.2ms\n",
            "Speed: 3.4ms preprocess, 14.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 8.5ms\n",
            "Speed: 3.3ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 truck, 3 traffic lights, 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 8.7ms\n",
            "Speed: 3.2ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 13.2ms\n",
            "Speed: 7.8ms preprocess, 13.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 8.2ms\n",
            "Speed: 3.3ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 truck, 3 traffic lights, 9.3ms\n",
            "Speed: 3.3ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 3 traffic lights, 7.0ms\n",
            "Speed: 3.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 7.3ms\n",
            "Speed: 2.6ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 10.4ms\n",
            "Speed: 3.2ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 10.0ms\n",
            "Speed: 3.1ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 truck, 3 traffic lights, 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.1ms\n",
            "Speed: 2.3ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 truck, 3 traffic lights, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 8.4ms\n",
            "Speed: 3.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 9.6ms\n",
            "Speed: 3.2ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 3 traffic lights, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 3 traffic lights, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 truck, 3 traffic lights, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 7.1ms\n",
            "Speed: 2.6ms preprocess, 7.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 3 traffic lights, 14.4ms\n",
            "Speed: 3.0ms preprocess, 14.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 9.4ms\n",
            "Speed: 3.8ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 truck, 3 traffic lights, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 9.2ms\n",
            "Speed: 3.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 truck, 3 traffic lights, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 1 truck, 3 traffic lights, 8.6ms\n",
            "Speed: 2.8ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 truck, 3 traffic lights, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 3 traffic lights, 10.3ms\n",
            "Speed: 3.3ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 3 traffic lights, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 3 traffic lights, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 8.6ms\n",
            "Speed: 3.2ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 9.7ms\n",
            "Speed: 3.2ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 8.3ms\n",
            "Speed: 3.0ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 7.1ms\n",
            "Speed: 3.1ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 3 traffic lights, 7.9ms\n",
            "Speed: 3.4ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 3 traffic lights, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 9.5ms\n",
            "Speed: 3.2ms preprocess, 9.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 15.5ms\n",
            "Speed: 9.2ms preprocess, 15.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 truck, 3 traffic lights, 11.4ms\n",
            "Speed: 3.2ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 truck, 3 traffic lights, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 1 truck, 3 traffic lights, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 10.7ms\n",
            "Speed: 4.2ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 8.9ms\n",
            "Speed: 3.7ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.8ms\n",
            "Speed: 2.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 7.7ms\n",
            "Speed: 2.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 8.4ms\n",
            "Speed: 3.1ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 bus, 1 truck, 3 traffic lights, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 8.5ms\n",
            "Speed: 3.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 truck, 3 traffic lights, 8.6ms\n",
            "Speed: 3.1ms preprocess, 8.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 3 traffic lights, 18.5ms\n",
            "Speed: 4.5ms preprocess, 18.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 3 traffic lights, 9.5ms\n",
            "Speed: 7.0ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 truck, 3 traffic lights, 7.9ms\n",
            "Speed: 3.3ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 truck, 4 traffic lights, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 bus, 3 traffic lights, 8.8ms\n",
            "Speed: 3.1ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 bus, 3 traffic lights, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 bus, 3 traffic lights, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 bus, 3 traffic lights, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 bus, 3 traffic lights, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 bus, 1 truck, 3 traffic lights, 10.4ms\n",
            "Speed: 3.1ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 bus, 3 traffic lights, 8.2ms\n",
            "Speed: 3.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 1 bus, 3 traffic lights, 8.3ms\n",
            "Speed: 2.9ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 bus, 1 truck, 3 traffic lights, 8.5ms\n",
            "Speed: 2.8ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 cars, 1 bus, 4 traffic lights, 10.4ms\n",
            "Speed: 3.0ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 cars, 1 bus, 1 truck, 4 traffic lights, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 bus, 1 truck, 4 traffic lights, 8.7ms\n",
            "Speed: 2.9ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 1 bus, 4 traffic lights, 8.6ms\n",
            "Speed: 2.8ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 1 bus, 1 truck, 4 traffic lights, 8.7ms\n",
            "Speed: 2.9ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 bus, 4 traffic lights, 8.8ms\n",
            "Speed: 2.9ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 bus, 4 traffic lights, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 10.5ms\n",
            "Speed: 3.1ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 8.2ms\n",
            "Speed: 2.9ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 4 traffic lights, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 4 traffic lights, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 9.3ms\n",
            "Speed: 2.9ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 4 traffic lights, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 4 traffic lights, 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 4 traffic lights, 7.6ms\n",
            "Speed: 2.9ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 8.5ms\n",
            "Speed: 3.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 12.3ms\n",
            "Speed: 3.0ms preprocess, 12.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 cars, 3 traffic lights, 12.3ms\n",
            "Speed: 3.9ms preprocess, 12.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 3 traffic lights, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 7.1ms\n",
            "Speed: 5.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 7.1ms\n",
            "Speed: 2.2ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 9.3ms\n",
            "Speed: 3.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 3 traffic lights, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 3 traffic lights, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 3 traffic lights, 7.8ms\n",
            "Speed: 3.8ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 8.2ms\n",
            "Speed: 3.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 8.3ms\n",
            "Speed: 3.2ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 8.0ms\n",
            "Speed: 3.3ms preprocess, 8.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 8.1ms\n",
            "Speed: 3.2ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 10.8ms\n",
            "Speed: 3.1ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 8.5ms\n",
            "Speed: 3.6ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.8ms\n",
            "Speed: 3.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.6ms\n",
            "Speed: 3.4ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 3 traffic lights, 7.4ms\n",
            "Speed: 2.7ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 3 traffic lights, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 9.0ms\n",
            "Speed: 3.4ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.6ms\n",
            "Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.4ms\n",
            "Speed: 2.6ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 10.0ms\n",
            "Speed: 3.3ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.2ms\n",
            "Speed: 3.9ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 8.8ms\n",
            "Speed: 2.6ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 10.2ms\n",
            "Speed: 3.0ms preprocess, 10.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 9.1ms\n",
            "Speed: 2.9ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 9.0ms\n",
            "Speed: 3.2ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 4 traffic lights, 8.7ms\n",
            "Speed: 3.0ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 8.1ms\n",
            "Speed: 2.6ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 8.1ms\n",
            "Speed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 7.5ms\n",
            "Speed: 3.4ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 3 traffic lights, 7.3ms\n",
            "Speed: 2.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 3 traffic lights, 13.4ms\n",
            "Speed: 3.6ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 3 traffic lights, 7.7ms\n",
            "Speed: 3.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 3 traffic lights, 7.8ms\n",
            "Speed: 3.4ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 8.9ms\n",
            "Speed: 3.2ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 3 traffic lights, 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 3 traffic lights, 8.9ms\n",
            "Speed: 3.7ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 10.5ms\n",
            "Speed: 3.2ms preprocess, 10.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 8.4ms\n",
            "Speed: 3.2ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 3 traffic lights, 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.1ms\n",
            "Speed: 2.7ms preprocess, 7.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 12.9ms\n",
            "Speed: 2.9ms preprocess, 12.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 3 traffic lights, 7.7ms\n",
            "Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 cars, 3 traffic lights, 7.2ms\n",
            "Speed: 3.2ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 cars, 3 traffic lights, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 3 traffic lights, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 7.5ms\n",
            "Speed: 2.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 3 traffic lights, 10.5ms\n",
            "Speed: 5.7ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 cars, 3 traffic lights, 8.6ms\n",
            "Speed: 3.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 cars, 3 traffic lights, 8.5ms\n",
            "Speed: 3.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 cars, 3 traffic lights, 8.1ms\n",
            "Speed: 3.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 3 traffic lights, 8.2ms\n",
            "Speed: 3.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 cars, 3 traffic lights, 8.1ms\n",
            "Speed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 cars, 3 traffic lights, 7.1ms\n",
            "Speed: 3.1ms preprocess, 7.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 7.2ms\n",
            "Speed: 2.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 8.9ms\n",
            "Speed: 3.1ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 3 traffic lights, 8.1ms\n",
            "Speed: 3.7ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 8.7ms\n",
            "Speed: 3.2ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 3 traffic lights, 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 9.7ms\n",
            "Speed: 3.1ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 12.4ms\n",
            "Speed: 3.1ms preprocess, 12.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 8.3ms\n",
            "Speed: 2.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 4 traffic lights, 7.2ms\n",
            "Speed: 4.7ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 8.6ms\n",
            "Speed: 3.3ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 7.6ms\n",
            "Speed: 3.2ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 4 traffic lights, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 8.4ms\n",
            "Speed: 3.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.9ms\n",
            "Speed: 3.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 cars, 3 traffic lights, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 3 traffic lights, 7.6ms\n",
            "Speed: 3.0ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 cars, 3 traffic lights, 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 3 traffic lights, 8.4ms\n",
            "Speed: 3.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 3 traffic lights, 9.1ms\n",
            "Speed: 2.8ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 3 traffic lights, 7.4ms\n",
            "Speed: 3.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.2ms\n",
            "Speed: 2.4ms preprocess, 7.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 3 traffic lights, 9.5ms\n",
            "Speed: 3.3ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.5ms\n",
            "Speed: 2.7ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 7.2ms\n",
            "Speed: 2.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 7.2ms\n",
            "Speed: 2.1ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 4 traffic lights, 8.5ms\n",
            "Speed: 2.9ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 4 traffic lights, 8.7ms\n",
            "Speed: 3.0ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 8.6ms\n",
            "Speed: 3.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 12.6ms\n",
            "Speed: 2.4ms preprocess, 12.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 7.2ms\n",
            "Speed: 2.2ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 4 traffic lights, 7.0ms\n",
            "Speed: 2.4ms preprocess, 7.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 4 traffic lights, 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 4 traffic lights, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 4 traffic lights, 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 4 traffic lights, 8.3ms\n",
            "Speed: 3.6ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 7.5ms\n",
            "Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 4 traffic lights, 9.5ms\n",
            "Speed: 3.2ms preprocess, 9.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 4 traffic lights, 8.8ms\n",
            "Speed: 3.2ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 4 traffic lights, 7.9ms\n",
            "Speed: 2.3ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 7.2ms\n",
            "Speed: 2.5ms preprocess, 7.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 9.6ms\n",
            "Speed: 2.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 7.5ms\n",
            "Speed: 2.6ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 7.3ms\n",
            "Speed: 2.5ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 4 traffic lights, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 8.1ms\n",
            "Speed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 4 traffic lights, 10.2ms\n",
            "Speed: 3.2ms preprocess, 10.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 8.8ms\n",
            "Speed: 3.1ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 10.8ms\n",
            "Speed: 3.6ms preprocess, 10.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 4 traffic lights, 7.3ms\n",
            "Speed: 2.2ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 3 traffic lights, 7.0ms\n",
            "Speed: 2.1ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 3 traffic lights, 9.0ms\n",
            "Speed: 3.7ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 3 traffic lights, 12.7ms\n",
            "Speed: 5.5ms preprocess, 12.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 9.1ms\n",
            "Speed: 3.4ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.3ms\n",
            "Speed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.8ms\n",
            "Speed: 4.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 10.3ms\n",
            "Speed: 3.0ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 3 traffic lights, 7.1ms\n",
            "Speed: 5.6ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 3 traffic lights, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 4 traffic lights, 12.0ms\n",
            "Speed: 3.2ms preprocess, 12.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 12.8ms\n",
            "Speed: 3.1ms preprocess, 12.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 4 traffic lights, 13.1ms\n",
            "Speed: 3.2ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 3 traffic lights, 10.3ms\n",
            "Speed: 3.2ms preprocess, 10.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 3 traffic lights, 13.9ms\n",
            "Speed: 3.1ms preprocess, 13.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 4 traffic lights, 10.4ms\n",
            "Speed: 3.2ms preprocess, 10.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 4 traffic lights, 11.8ms\n",
            "Speed: 3.1ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 4 traffic lights, 9.4ms\n",
            "Speed: 3.7ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 4 traffic lights, 10.7ms\n",
            "Speed: 6.2ms preprocess, 10.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 4 traffic lights, 9.5ms\n",
            "Speed: 3.9ms preprocess, 9.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 4 traffic lights, 16.7ms\n",
            "Speed: 3.1ms preprocess, 16.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 4 traffic lights, 11.9ms\n",
            "Speed: 3.4ms preprocess, 11.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 4 traffic lights, 10.5ms\n",
            "Speed: 3.1ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 4 traffic lights, 10.4ms\n",
            "Speed: 3.1ms preprocess, 10.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 4 traffic lights, 11.3ms\n",
            "Speed: 3.2ms preprocess, 11.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 4 traffic lights, 8.6ms\n",
            "Speed: 4.3ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "âœ… Done! Download your output video below.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ba8f05c2-0fa9-4845-9c8c-af1f9d9412d9\", \"output_redlightcross.mp4\", 13877206)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interface"
      ],
      "metadata": {
        "id": "ktW4Wkl2zFZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import cv2\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "st.title(\"Traffic Rule Violation Detection\")\n",
        "st.markdown(\"Upload a video and select the detection type:\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose a video file\", type=[\"mp4\", \"avi\", \"mov\"])\n",
        "\n",
        "option = st.radio(\n",
        "    \"Select Detection Option\",\n",
        "    (\"Helmet Detection\", \"Red Light Crossing\", \"Triple Riding\")\n",
        ")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Save uploaded file to temp\n",
        "    tfile = tempfile.NamedTemporaryFile(delete=False)\n",
        "    tfile.write(uploaded_file.read())\n",
        "    video_path = tfile.name\n",
        "\n",
        "    st.video(video_path)\n",
        "\n",
        "    st.write(f\"Running {option}...\")\n",
        "\n",
        "    # Placeholder: Your detection code goes here\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Save output to temp video\n",
        "    output_path = tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False).name\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Dummy processing (draw text)\n",
        "        cv2.putText(frame, f\"{option} Processing...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    st.success(f\"{option} completed.\")\n",
        "    st.video(output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FHycps0zBww",
        "outputId": "0d821201-8e5d-449e-a2b5-92b9a1bc8cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok streamlit opencv-python-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-mK1c_CJzINQ",
        "outputId": "5009799e-eb86-4311-a48a-f69f4120d65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.47.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Downloading streamlit-1.47.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.2.12 streamlit-1.47.0 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Authenticate Ngrok (use your own token)\n",
        "!ngrok authtoken 2wUKvkLwdo1dxyB6WbIO78OqSYy_DyKyKHnsGB3n8HHuEv8\n",
        "\n",
        "# Kill any previous Streamlit processes\n",
        "!pkill -f streamlit || echo \"No streamlit process\"\n",
        "# Kill any previous ngrok processes\n",
        "!pkill -f ngrok || echo \"No ngrok process\"\n",
        "\n",
        "# Run streamlit app\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\"])\n",
        "time.sleep(5)\n",
        "\n",
        "# Connect ngrok to the Streamlit app\n",
        "url = ngrok.connect(\"8501\", \"http\")\n",
        "print(f\"Streamlit app running at {url}\")"
      ],
      "metadata": {
        "id": "bsBcwlLkz20Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit\n",
        "!pkill -f ngrok"
      ],
      "metadata": {
        "id": "Tv5JMTGizl3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit\n",
        "!pkill -f ngrok"
      ],
      "metadata": {
        "id": "eqM8kO1KzXFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}